{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f616a39-4fe2-4d50-973b-3110e2c1723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ec365-6966-4442-afd6-6f0d27c666a5",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e309bd-056d-41d9-80ed-f5461d9bd545",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "734718b9-3c51-42d8-947e-62a2212e4344",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_files/sessions.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_cleaning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clean_sessions_jsonl\n\u001b[0;32m----> 3\u001b[0m \u001b[43mclean_sessions_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_files/tracks.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_files/sessions.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_files/sessions_clean.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/playlist_recommendation_system/data/data_cleaning.py:22\u001b[0m, in \u001b[0;36mclean_sessions_jsonl\u001b[0;34m(tracks_filepath, sessions_filepath, output_filepath, chunk_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_sessions_jsonl\u001b[39m(tracks_filepath: \u001b[38;5;28mstr\u001b[39m, sessions_filepath: \u001b[38;5;28mstr\u001b[39m, output_filepath:\u001b[38;5;28mstr\u001b[39m, chunk_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     tracks_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(tracks_filepath, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msessions_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin, \u001b[38;5;28mopen\u001b[39m(output_filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fout:\n\u001b[1;32m     23\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m fin:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_files/sessions.jsonl'"
     ]
    }
   ],
   "source": [
    "from data.data_cleaning import clean_sessions_jsonl\n",
    "\n",
    "clean_sessions_jsonl('data_files/tracks.jsonl', 'data_files/sessions.jsonl', 'data_files/sessions_clean.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e70505-353c-4b27-b121-76dad58c24e2",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c4fb9b-4f0d-48cc-b414-614f56aebc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_split import split_data\n",
    "\n",
    "sessions_df = pd.read_json('data_files/sessions_clean.jsonl', lines=True)\n",
    "train_data_df, val_data_df, test_data_df = split_data(sessions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f51636-536b-4396-9596-3d3c258e39a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Interaction Score Aggregation\n",
    "\n",
    "In this step, we transform raw user-track interaction events into aggregated scores that will be used for matrix factorization. The transformation process:\n",
    "\n",
    "1. Groups all events by unique user-track pairs\n",
    "2. Calculates a single interaction score for each pair based on event types:\n",
    "  - 'like' events contribute 1.0 to the score\n",
    "  - 'play' events contribute 1.0 to the score\n",
    "  - 'skip' events and other events contribute 0.0\n",
    "\n",
    "This converts sequential interaction logs (with timestamps and session IDs) into a format suitable for collaborative filtering - where each user-track pair has a single numerical score representing the user's overall interest level in that track. These scores will be used to train the matrix factorization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386f774a-920e-40d1-aed2-ed28c80d8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_transforming import aggregate_interactions\n",
    "\n",
    "train_data_aggregated_df = aggregate_interactions(train_data_df)\n",
    "val_data_aggregated_df = aggregate_interactions(val_data_df)\n",
    "test_data_aggregated_df = aggregate_interactions(test_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b4cf2-adaf-4434-bc2b-069fe49cb3fe",
   "metadata": {},
   "source": [
    "### Saving the datasets for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44335e43-a433-436d-a529-dd855027b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_aggregated_df.to_json('data_files/train_sessions.jsonl', orient='records', lines=True)\n",
    "val_data_aggregated_df.to_json('data_files/val_sessions.jsonl', orient='records', lines=True)\n",
    "test_data_aggregated_df.to_json('data_files/test_sessions.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b0b1a-3adb-456c-88cf-114eafdc38b5",
   "metadata": {},
   "source": [
    "### Loading the preprocessed training and validation session data from JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6355c369-dfe1-4141-95ec-4d6dd3550cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_train_df = pd.read_json('data_files/train_sessions.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e2c5b9-174f-4ec4-8907-4d502e6020e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_val_df = pd.read_json('data_files/val_sessions.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bac09-5800-4fd1-84ec-b80af74e74f6",
   "metadata": {},
   "source": [
    "# Basic model\n",
    "\n",
    "The basic model follows a straightforward approach to generate group recommendations: for each user in the group, it identifies their top 50 most played tracks based on historical listening data. It then combines these tracks across all group members and prioritizes songs that appear in multiple users' top tracks. When recommending N tracks, the algorithm first selects songs that are popular among the most group members, then fills any remaining slots randomly from the combined pool of top tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb439669-d886-465c-bff9-5567d64933f7",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98f41dd5-f0c2-49a7-abde-a87128088021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.core.basic_model import BasicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df5ca248-2f07-4d36-ac3e-3d4471d5f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = BasicModel('data_files/train_sessions.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6668a8-c5d9-4424-abc5-c3f36b9f0641",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c12ea-eed3-4720-a763-39a54316ae7a",
   "metadata": {},
   "source": [
    "1. Relevant Tracks:\n",
    "    - For a user group, get all tracks from validation set where:\n",
    "      * track was listened to by ANY user from the group\n",
    "      * interaction score >= 1.0\n",
    "\n",
    "2. Precision Calculation:\n",
    "    precision = (number of recommended tracks that appear in relevant tracks) / (total recommendations)\n",
    "    \n",
    "    Example: If 3 out of 10 recommended tracks were listened to by any group \n",
    "    member in validation set, precision = 0.3 (30%)\n",
    "\n",
    "3. Final Metrics:\n",
    "    - Calculate for different recommendation sizes (5, 10, 30 tracks)\n",
    "    - Take 10 random groups of 5 users each\n",
    "    - Report mean precision and standard deviation for each size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d995b094-d148-4bec-b731-43b7fbc9bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set, Tuple\n",
    "\n",
    "def get_group_relevant_tracks(val_df: pd.DataFrame, \n",
    "                            user_ids: List[int],\n",
    "                            relevance_threshold: float = 1.0) -> Set[int]:\n",
    "    relevant_interactions = val_df[\n",
    "        (val_df['user_id'].isin(user_ids)) & \n",
    "        (val_df['score'] >= relevance_threshold)\n",
    "    ]\n",
    "    return set(relevant_interactions['track_id'].unique())\n",
    "\n",
    "def calculate_group_precision(predicted_tracks: List[str],\n",
    "                          relevant_tracks: Set[int]) -> float:\n",
    "    predicted_set = set(predicted_tracks)\n",
    "    true_positives = len(relevant_tracks.intersection(predicted_set))\n",
    "    \n",
    "    return true_positives / len(predicted_set) if predicted_set else 0.0\n",
    "\n",
    "def evaluate_model_groups(model,\n",
    "                        val_df: pd.DataFrame,\n",
    "                        n_groups: int = 10,\n",
    "                        group_size: int = 5,\n",
    "                        n_recommendations: List[int] = [5, 10, 30],\n",
    "                        relevance_threshold: float = 1.0) -> Dict:\n",
    "    metrics = {n: [] for n in n_recommendations}\n",
    "    \n",
    "    all_users = val_df['user_id'].unique().tolist()\n",
    "    for _ in range(n_groups):\n",
    "        group_users = np.random.choice(all_users, size=group_size, replace=False)\n",
    "        \n",
    "        relevant_tracks = get_group_relevant_tracks(\n",
    "            val_df, group_users, relevance_threshold\n",
    "        )\n",
    "        \n",
    "        if not relevant_tracks:\n",
    "            continue\n",
    "            \n",
    "        for n in n_recommendations:\n",
    "            recs = model.get_recommendations(\n",
    "                [str(uid) for uid in group_users], \n",
    "                n\n",
    "            )\n",
    "            recs = [track_id for track_id in recs]\n",
    "            \n",
    "            precision = calculate_group_precision(recs, relevant_tracks)\n",
    "            metrics[n].append(precision)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_group_metrics(metrics: Dict, n_recommendations: List[int]) -> None:\n",
    "    print(\"\\nGroup Recommendations Performance:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for n in n_recommendations:\n",
    "        print(f\"\\nPrecision@{n}: {np.mean(metrics[n]):.3f} ± {np.std(metrics[n]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a63ecc-5aa4-433f-ba03-13a5863d79e1",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c56ffb6-4e40-4d41-b853-6175db8fe70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group Recommendations Performance:\n",
      "--------------------------------------------------\n",
      "\n",
      "Precision@5: 0.145 ± 0.131\n",
      "\n",
      "Precision@10: 0.152 ± 0.094\n",
      "\n",
      "Precision@30: 0.177 ± 0.105\n"
     ]
    }
   ],
   "source": [
    "group_metrics_for_basic_model = evaluate_model_groups(\n",
    "    basic_model,\n",
    "    sessions_val_df,\n",
    "    n_groups=10,\n",
    "    group_size=5,\n",
    "    n_recommendations=[5, 10, 30]\n",
    ")\n",
    "\n",
    "print_group_metrics(group_metrics_for_basic_model, [5, 10, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c11c6e-9386-4fb0-83fd-20683f14c6ad",
   "metadata": {},
   "source": [
    "### Unique User-Track Combinations In Validation Set\n",
    "Our basic model can only recommend tracks users have already listened to, which is a significant limitation since 75.43% of user-track interactions in the validation set are completely new combinations not present in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c3fc562-0635-4374-b5e9-5185790f6d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.43% of user-track pairs in the validation set are not present in the training set\n"
     ]
    }
   ],
   "source": [
    "train_combinations = set(zip(sessions_train_df['user_id'], sessions_train_df['track_id']))\n",
    "val_combinations = set(zip(sessions_val_df['user_id'], sessions_val_df['track_id']))\n",
    "\n",
    "unique_to_val = val_combinations - train_combinations\n",
    "\n",
    "print(f\"{len(unique_to_val)/len(val_combinations)*100:.2f}% of user-track pairs in the validation set are not present in the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc81c52-4d1f-48b3-b4be-bf61e6cb01e6",
   "metadata": {},
   "source": [
    "## Basic Model Evaluation Summary\n",
    "Looking at the basic model's performance metrics, the results are surprisingly decent given that the model simply recommends tracks that group members have previously listened to, without any predictive capabilities.\n",
    "This is particularly interesting considering that 75.43% of user-track interactions in the validation set don't exist in the training data. The relatively good performance despite this limitation suggests that users tend to repeatedly listen to certain tracks over time, making historical listening data a useful signal even without sophisticated prediction mechanisms.\n",
    "From a business perspective, this simple approach has a key advantage: users are likely to be satisfied when their favorite tracks appear in group playlists. While more sophisticated models might discover new music more effectively, including well-loved tracks from users' listening history can help create a comfortable group listening experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e8686-9af8-4c52-988e-eac8f4dabc4f",
   "metadata": {},
   "source": [
    "# Advanced Model\n",
    "Advanced Model\n",
    "The advanced model implements collaborative filtering using matrix factorization (WMF - Weighted Matrix Factorization), which was thoroughly described in Phase 1 of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16eea5c-9756-49e4-a060-ceb5d86f02b6",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Before working with the WMF model, we need to properly encode user and track IDs. The model uses embedding layers which require continuous zero-based indices (0 to n-1) for lookup operations. Since our dataset contains non-sequential IDs (e.g., user_ids like 1001, 1578, 2403), we use `IDEncoder` object to map them to the required format. The encoders are saved alongside the model to ensure consistent ID mapping between training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c11fc-f433-45a7-aed7-db26fc11deaf",
   "metadata": {},
   "source": [
    "### Use when you would like to initialize new model ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e14b95f-1221-4429-84fc-d29ef5ddcac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.wmf import WMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82f250da-02b0-4385-81c5-e4b79c96ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_transforming import IDEncoder\n",
    "\n",
    "users_df = pd.read_json('data_files/users.jsonl', lines=True)\n",
    "tracks_df = pd.read_json('data_files/tracks.jsonl', lines=True)\n",
    "user_encoder = IDEncoder()\n",
    "track_encoder = IDEncoder()\n",
    "user_encoder.fit(users_df[\"user_id\"].unique().tolist())\n",
    "track_encoder.fit(tracks_df[\"id\"].unique().tolist())\n",
    "model = WMF(\n",
    "     n_users=len(user_encoder), \n",
    "     n_items=len(track_encoder), \n",
    "     embedding_dim=32, \n",
    "     dropout_rate=0.0, \n",
    "     init=True, \n",
    "     bias=False, \n",
    "     sigmoid=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e85d8-de36-43eb-9fc7-5ab524101896",
   "metadata": {},
   "source": [
    "### Use when you would like to load model from file ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2f817b6-eb27-4db4-9651-d64a02774b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = torch.load(\n",
    "    './model_files/wmf_model.pth',\n",
    "    map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "user_encoder = model_data[\"user_encoder\"]\n",
    "track_encoder = model_data[\"track_encoder\"]\n",
    "config = model_data[\"model_config\"]\n",
    "model = WMF(\n",
    "    n_users=len(user_encoder),\n",
    "    n_items=len(track_encoder),\n",
    "    embedding_dim=config[\"embedding_dim\"],\n",
    "    dropout_rate=config[\"dropout_rate\"],\n",
    "    bias=config[\"bias\"],\n",
    "    sigmoid=config[\"sigmoid\"],\n",
    "    init=False\n",
    ")\n",
    "model.load_state_dict(model_data[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ca268-bb70-4e2a-8182-3e7ab81a3446",
   "metadata": {},
   "source": [
    "### Using our previously created encoders to transform user and track IDs into continuous zero-based indices required by the embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51fa2ef1-891c-40fa-a9e1-53792c5977bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_train_df[\"user_id\"] = user_encoder.encode(sessions_train_df[\"user_id\"].tolist())\n",
    "sessions_train_df[\"track_id\"] = track_encoder.encode(sessions_train_df[\"track_id\"].tolist())\n",
    "sessions_val_df[\"user_id\"] = user_encoder.encode(sessions_val_df[\"user_id\"].tolist())\n",
    "sessions_val_df[\"track_id\"] = track_encoder.encode(sessions_val_df[\"track_id\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f458f32-e0c0-46ec-9ec4-99bd9b717984",
   "metadata": {},
   "source": [
    "### Generating Complete User-Track Interaction Matrix\n",
    "\n",
    "We create a complete interaction matrix using a subset of users (IDs 2000-2499) and all tracks. We take only a subset of users to reduce training time. First, we generate all possible user-track pairs using cartesian product, then merge this with our existing interaction data, filling missing scores with zeros (indicating no interaction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b9c516e-a2b9-44dd-9eed-8024f52fc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "users_range = np.arange(2000, 2500)\n",
    "tracks_range = np.arange(len(track_encoder))\n",
    "df = pd.DataFrame(\n",
    "    product(users_range, tracks_range),\n",
    "    columns=['user_id', 'track_id']\n",
    ")\n",
    "\n",
    "sessions_train_df = df.merge(\n",
    "    sessions_train_df[['user_id', 'track_id', 'score']],\n",
    "    on=['user_id', 'track_id'],\n",
    "    how='left'\n",
    ").fillna(0)\n",
    "\n",
    "sessions_val_df = df.merge(\n",
    "    sessions_val_df[['user_id', 'track_id', 'score']],\n",
    "    on=['user_id', 'track_id'], \n",
    "    how='left'\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d0f47-b73f-40f2-b298-46fb33a6fcc4",
   "metadata": {},
   "source": [
    "### Calculating Preferences and Confidence Weights\n",
    "\n",
    "Following the collaborative filtering algorithm, we transform each user-track interaction into two values:\n",
    "\n",
    "1. Preference:\n",
    "  - Set to 1 if score ≥ 1 (user interacted positively with track)\n",
    "  - Set to 0 if score = 0 (no interaction)\n",
    "\n",
    "2. Confidence Weight:\n",
    "  - Calculated using the formula: 1 + α * log(1 + score/ε)\n",
    "  - Where:\n",
    "    - α (alpha) = 40: hyperparameter controlling confidence scaling\n",
    "    - ε (epsilon) = 1e-8: small constant to prevent log(0)\n",
    "  - Higher interaction scores result in higher confidence values\n",
    "\n",
    "These values are then used in the model's objective function:\n",
    "\n",
    "$$\n",
    "\\min_{x,y} \\sum_{u,i} c_{ui}(p_{ui} - x_u^Ty_i)^2 + \\lambda(\\sum_u ||x_u||^2 + \\sum_i ||y_i||^2)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $c_{ui}$: confidence weight for interaction between user $u$ and track $i$\n",
    "- $p_{ui}$: preference value (0 or 1) for user $u$ and track $i$\n",
    "- $x_u$: embedding vector for user $u$\n",
    "- $y_i$: embedding vector for track $i$\n",
    "- $x_u^Ty_i$: dot product of user and track embeddings (predicted preference)\n",
    "- $\\lambda$: regularization parameter controlling the strength of regularization\n",
    "- $||x_u||^2$, $||y_i||^2$: L2 norms of embedding vectors (regularization terms)\n",
    "\n",
    "In our implementation, we use L2 regularization through the AdamW optimizer's weight decay parameter, which achieves the same effect of preventing overfitting by penalizing large embedding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c915386-c185-4927-8d2f-9ed1ec97f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_train_df['preference'] = sessions_train_df['score'] >= 1\n",
    "sessions_val_df['preference'] = sessions_val_df['score'] >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72688251-409f-46de-95e3-f81ac5a339f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 40\n",
    "epsilon = 1e-8\n",
    "sessions_train_df['weight'] = 1 + alpha * np.log(1 + sessions_train_df['score'] / epsilon)\n",
    "sessions_val_df['weight'] = 1 + alpha * np.log(1 + sessions_val_df['score'] / epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a297b08-a1e0-47a9-9181-3c1b94aa2287",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28cd2f5f-b277-482d-8954-37c2af386135",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.LongTensor(sessions_train_df['user_id'].values),\n",
    "    torch.LongTensor(sessions_train_df['track_id'].values),\n",
    "    torch.FloatTensor(sessions_train_df['preference'].values),\n",
    "    torch.FloatTensor(sessions_train_df['weight'].values)\n",
    ")\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.LongTensor(sessions_val_df['user_id'].values),\n",
    "    torch.LongTensor(sessions_val_df['track_id'].values),\n",
    "    torch.FloatTensor(sessions_val_df['preference'].values),\n",
    "    torch.FloatTensor(sessions_val_df['weight'].values)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46fc7882-cf69-49e9-a234-82ca20db6ecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, train_loader, val_loader, device)\n\u001b[0;32m----> 4\u001b[0m train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_with_weight_decay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/playlist_recommendation_system/models/training.py:54\u001b[0m, in \u001b[0;36mTrainer.train_with_weight_decay\u001b[0;34m(self, epochs, learning_rate, weight_decay)\u001b[0m\n\u001b[1;32m     51\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(predictions, labels, weights)\n\u001b[1;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 54\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     57\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(predictions, labels, weights)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.training import Trainer\n",
    "\n",
    "trainer = Trainer(model, train_loader, val_loader, device)\n",
    "train_loss, val_loss = trainer.train_with_weight_decay(\n",
    "    epochs=15,\n",
    "    learning_rate=0.005,\n",
    "    weight_decay=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e06199-d61b-4ad4-aff7-e36a0abdfd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, 'b-', label='Training Loss')\n",
    "plt.plot(val_loss, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095838fd-2244-497a-90aa-c81f0f52a7ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training Results Analysis\n",
    "\n",
    "The training process shows an interesting and unusual pattern where the validation loss is consistently lower than the training loss. After multiple code reviews and verifications of the implementation, we couldn't identify any bugs that would cause this behavior.\n",
    "\n",
    "This counterintuitive result might be explained by the data characteristics of our problem:\n",
    "1. We're training on all tracks from tracks.json, but users interacted with only a small subset of them (this will be explored later)\n",
    "2. This creates a highly sparse interaction matrix with many zero values\n",
    "3. Combined with:\n",
    "  - The large number of model parameters (embeddings for all users and tracks)\n",
    "  - Relatively small amount of actual interaction data\n",
    "  - The complexity of gradient descent optimization\n",
    "  \n",
    "This sparsity and parameter-to-data ratio could potentially lead to this unusual loss pattern. However, without further investigation, we cannot make a definitive conclusion about the cause of this behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ab17d-b032-4a7d-bf3b-0d009348cc85",
   "metadata": {},
   "source": [
    "### Regularization Analysis\n",
    "\n",
    "To verify if our model's regularization is working correctly, we examined the norms of the learned embeddings:\n",
    "- Average user embedding norm: 0.34\n",
    "- Average item embedding norm: 0.98\n",
    "\n",
    "These values indicate that the regularization (implemented through AdamW's weight decay) is effectively preventing the embeddings from growing too large. This suggests that the unusual training/validation loss pattern is not caused by insufficient regularization, and we should look for other explanations in the data characteristics and model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ef7a080-a327-474f-b3f7-bf34cfdcfaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average user embedding norm: 0.31869420409202576\n",
      "Average item embedding norm: 0.9587802886962891\n"
     ]
    }
   ],
   "source": [
    "user_norms = torch.norm(model.user_embedding.weight, dim=1)\n",
    "avg_user_norm = torch.mean(user_norms) \n",
    "item_norms = torch.norm(model.item_embedding.weight, dim=1)\n",
    "avg_item_norm = torch.mean(item_norms)\n",
    "\n",
    "print(\"Average user embedding norm:\", avg_user_norm.item())\n",
    "print(\"Average item embedding norm:\", avg_item_norm.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f153c-627b-47f3-b5cb-69372ca5ae86",
   "metadata": {},
   "source": [
    "### Saving the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5753b4aa-9d40-431f-bf38-9848fe93ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'embedding_dim': model.user_embedding.embedding_dim,\n",
    "        'dropout_rate': model.dropout.p,\n",
    "        'bias': model.bias,\n",
    "        'sigmoid': model.sigmoid,\n",
    "    },\n",
    "    'user_encoder': user_encoder,\n",
    "    'track_encoder': track_encoder\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, './model_files/wmf_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae7302-567a-4231-8c22-ea47efe3e4c6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155bf44a-ff03-4966-8727-86672c33d5ce",
   "metadata": {},
   "source": [
    "### Preparing Evaluation Data\n",
    "\n",
    "For evaluation purposes, we select a random subset of 50 users. For these users, we generate all possible user-track pairs to evaluate how well our model ranks all available tracks for each user.\n",
    "\n",
    "The process involves several steps:\n",
    "1. Select 50 random users from our user base using numpy's random choice function\n",
    "2. Create a cartesian product of these users with all available tracks in our dataset\n",
    "3. Merge this complete set of user-track pairs with our validation data (which contains actual user interactions)\n",
    "4. Fill missing values with 0 (representing no interaction)\n",
    "\n",
    "This approach assumes that the validation data (`sessions_val_df`) has already been aggregated using the `aggregate_interactions` function, which was done in the data preparation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "533dc57d-1b4e-4898-b644-b315757728ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ids = np.random.choice(len(user_encoder), size=50, replace=False)\n",
    "users = np.array(random_ids)\n",
    "tracks = np.arange(len(track_encoder))\n",
    "df = pd.DataFrame(\n",
    "    product(users, tracks),\n",
    "    columns=['user_id', 'track_id']\n",
    ")\n",
    "\n",
    "sessions_val_df = df.merge(\n",
    "    sessions_val_df[['user_id', 'track_id', 'score']],\n",
    "    on=['user_id', 'track_id'], \n",
    "    how='left'\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a446f23-bee3-4608-8f91-df9d772a5b83",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "140dce07-0aa9-473f-ae15-f39ffc0e1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.LongTensor(sessions_val_df['user_id'].values),\n",
    "    torch.LongTensor(sessions_val_df['track_id'].values),\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "100cb7a2-8dcc-4796-a9a1-406d7aed3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.predict import predict\n",
    "sessions_val_df['score_pred'] = predict(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e836ee7c-0b5a-4c70-948b-597de1c21c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [5, 10, 30]\n",
    "\n",
    "from evaluation.evaluator import Evaluator\n",
    "evaluator = Evaluator(k_list, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24749618-bee7-43cf-b1dd-927d6c6c64c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluator.evaluate(sessions_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad63a1-43de-4442-af53-3fb7a075599f",
   "metadata": {},
   "source": [
    "## Overall Model Performance\n",
    "Looking at the mean metric values, we can see that the model performs only slightly better than random recommendations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "734ee510-487e-45eb-8f53-208e851d3ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics:\n",
      "==================================================\n",
      "\n",
      "Metrics for top-5 recommendations:\n",
      "-----------------------------------\n",
      "NDCG       =   3.47% ±   8.06%\n",
      "Precision  =   2.40% ±   9.60%\n",
      "Recall     =   0.84% ±   4.76%\n",
      "\n",
      "Metrics for top-10 recommendations:\n",
      "-----------------------------------\n",
      "NDCG       =   3.90% ±   8.35%\n",
      "Precision  =   3.40% ±  12.22%\n",
      "Recall     =   1.28% ±   5.12%\n",
      "\n",
      "Metrics for top-30 recommendations:\n",
      "-----------------------------------\n",
      "NDCG       =   4.52% ±   9.21%\n",
      "Precision  =   3.07% ±  10.36%\n",
      "Recall     =   3.52% ±   9.35%\n"
     ]
    }
   ],
   "source": [
    "from evaluation.metrics import print_metrics\n",
    "print_metrics(metrics, k_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8cae4-a935-4abe-b7bf-8987c5a6c09b",
   "metadata": {},
   "source": [
    "## Per user metrics display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d575a3b3-4990-427e-ad17-d30be64060c9",
   "metadata": {},
   "source": [
    "To investigate model performance, we displayed metrics for each individual user. The results showed that most users had metrics equal to zero, while a few users (e.g., user 979 with NDCG@5 = 0.619 and Precision@5 = 1.0) showed exceptionally high performance. This interesting pattern led us to investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0942f1b1-54f9-4058-b1cf-30243267fbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>NDCG@30</th>\n",
       "      <th>Precision@30</th>\n",
       "      <th>Recall@30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.267229</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.189071</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.143277</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.279704</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.193275</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.228204</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.134328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0.084790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088754</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.062884</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>0.084790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>0.304712</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.224018</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.229160</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>0.138645</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.228354</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.204433</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.121212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>0.548130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.586283</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.123288</td>\n",
       "      <td>0.549265</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.301370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>0.300466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>0.054537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>0.443725</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.490894</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.561422</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.302632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>0.588874</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.547431</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.584244</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2212</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>0.169580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>0.339160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>0.290493</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.356040</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.410122</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.241935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2775</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2781</th>\n",
       "      <td>0.065603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026583</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.015987</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.011905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           NDCG@5  Precision@5  Recall@5   NDCG@10  Precision@10  Recall@10  \\\n",
       "user_id                                                                       \n",
       "35       0.040300          0.0  0.000000  0.030700           0.0   0.000000   \n",
       "97       0.000000          0.0  0.000000  0.041738           0.0   0.000000   \n",
       "121      0.267229          0.2  0.017241  0.189071           0.1   0.017241   \n",
       "144      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "199      0.279704          0.4  0.029851  0.193275           0.2   0.029851   \n",
       "267      0.000000          0.0  0.000000  0.054040           0.0   0.000000   \n",
       "371      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "372      0.000000          0.0  0.000000  0.047059           0.0   0.000000   \n",
       "446      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "580      0.084790          0.0  0.000000  0.057908           0.0   0.000000   \n",
       "584      0.000000          0.0  0.000000  0.088754           0.1   0.166667   \n",
       "606      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "627      0.084790          0.0  0.000000  0.055023           0.0   0.000000   \n",
       "788      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "863      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "935      0.000000          0.0  0.000000  0.052144           0.0   0.000000   \n",
       "948      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1059     0.304712          0.2  0.166667  0.224018           0.1   0.166667   \n",
       "1084     0.138645          0.4  0.030303  0.228354           0.4   0.060606   \n",
       "1089     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1150     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1194     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1204     0.548130          1.0  0.068493  0.586283           0.9   0.123288   \n",
       "1230     0.300466          0.0  0.000000  0.211033           0.0   0.000000   \n",
       "1255     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1546     0.054537          0.0  0.000000  0.038845           0.0   0.000000   \n",
       "1551     0.443725          1.0  0.065789  0.490894           0.8   0.105263   \n",
       "1641     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1697     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1793     0.000000          0.0  0.000000  0.057314           0.0   0.000000   \n",
       "1838     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2048     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2064     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2084     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2131     0.588874          0.6  0.100000  0.547431           0.5   0.166667   \n",
       "2183     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2212     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2250     0.169580          0.0  0.000000  0.110046           0.0   0.000000   \n",
       "2264     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2303     0.339160          0.0  0.000000  0.220092           0.0   0.000000   \n",
       "2382     0.290493          0.4  0.032258  0.356040           0.6   0.096774   \n",
       "2583     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2635     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2724     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2771     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2775     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2781     0.065603          0.0  0.000000  0.042572           0.0   0.000000   \n",
       "2795     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2803     0.000000          0.0  0.000000  0.026583           0.1   0.011905   \n",
       "2905     0.000000          0.0  0.000000  0.098336           0.0   0.000000   \n",
       "\n",
       "          NDCG@30  Precision@30  Recall@30  \n",
       "user_id                                     \n",
       "35       0.060620      0.000000   0.000000  \n",
       "97       0.026446      0.000000   0.000000  \n",
       "121      0.143277      0.066667   0.034483  \n",
       "144      0.000000      0.000000   0.000000  \n",
       "199      0.228204      0.300000   0.134328  \n",
       "267      0.104740      0.000000   0.000000  \n",
       "371      0.000000      0.000000   0.000000  \n",
       "372      0.040406      0.000000   0.000000  \n",
       "446      0.000000      0.000000   0.000000  \n",
       "580      0.071323      0.000000   0.000000  \n",
       "584      0.062884      0.033333   0.166667  \n",
       "606      0.000000      0.000000   0.000000  \n",
       "627      0.027288      0.000000   0.000000  \n",
       "788      0.000000      0.000000   0.000000  \n",
       "863      0.007853      0.000000   0.000000  \n",
       "935      0.055898      0.000000   0.000000  \n",
       "948      0.000000      0.000000   0.000000  \n",
       "1059     0.229160      0.066667   0.333333  \n",
       "1084     0.204433      0.266667   0.121212  \n",
       "1089     0.000000      0.000000   0.000000  \n",
       "1150     0.023504      0.000000   0.000000  \n",
       "1194     0.000000      0.000000   0.000000  \n",
       "1204     0.549265      0.733333   0.301370  \n",
       "1230     0.159768      0.000000   0.000000  \n",
       "1255     0.000000      0.000000   0.000000  \n",
       "1546     0.021191      0.000000   0.000000  \n",
       "1551     0.561422      0.766667   0.302632  \n",
       "1641     0.000000      0.000000   0.000000  \n",
       "1697     0.000000      0.000000   0.000000  \n",
       "1793     0.102544      0.000000   0.000000  \n",
       "1838     0.024201      0.000000   0.000000  \n",
       "2048     0.000000      0.000000   0.000000  \n",
       "2064     0.023785      0.000000   0.000000  \n",
       "2084     0.000000      0.000000   0.000000  \n",
       "2131     0.584244      0.366667   0.366667  \n",
       "2183     0.021306      0.000000   0.000000  \n",
       "2212     0.000000      0.000000   0.000000  \n",
       "2250     0.054576      0.000000   0.000000  \n",
       "2264     0.000000      0.000000   0.000000  \n",
       "2303     0.110367      0.000000   0.000000  \n",
       "2382     0.410122      0.500000   0.241935  \n",
       "2583     0.000000      0.000000   0.000000  \n",
       "2635     0.000000      0.000000   0.000000  \n",
       "2724     0.000000      0.000000   0.000000  \n",
       "2771     0.000000      0.000000   0.000000  \n",
       "2775     0.023167      0.000000   0.000000  \n",
       "2781     0.035082      0.000000   0.000000  \n",
       "2795     0.000000      0.000000   0.000000  \n",
       "2803     0.015987      0.033333   0.011905  \n",
       "2905     0.048769      0.000000   0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame()\n",
    "for k in k_list:\n",
    "    metrics_df[f'NDCG@{k}'] = metrics[f'NDCG@{k}']\n",
    "    metrics_df[f'Precision@{k}'] = metrics[f'Precision@{k}']\n",
    "    metrics_df[f'Recall@{k}'] = metrics[f'Recall@{k}']\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36f456-557b-4676-8239-fbd558461278",
   "metadata": {},
   "source": [
    "### Analysis of Model Performance Variations\n",
    "We compared two users - one with high metrics (user_id 2342, NDCG@5 = 0.619) and one with near-zero metrics (user_id 2607, NDCG@5 = 0) - to understand why only a few users had high metric scores. Our analysis revealed that for users with high metrics, the tracks they listened to in the validation set were also present in their training set interactions. For users with near-zero metrics, there was little to no overlap between their training and validation tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40bff8ed-ce3e-451d-9ca1-af2b740ca0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User 1204:\n",
      "Sum of score_val: 430.0\n",
      "Sum of score_train: 1144\n",
      "\n",
      "User 2905:\n",
      "Sum of score_val: 18.0\n",
      "Sum of score_train: 364\n"
     ]
    }
   ],
   "source": [
    "merged_df = sessions_val_df.merge(\n",
    "    sessions_train_df, \n",
    "    on=['user_id', 'track_id'], \n",
    "    how='right',\n",
    "    suffixes=('_val', '_train')\n",
    ")\n",
    "\n",
    "for user_id in [1204, 2905]:\n",
    "    print(f\"\\nUser {user_id}:\")\n",
    "    user_data = merged_df[merged_df['user_id'] == user_id]\n",
    "    print(f\"Sum of score_val: {user_data['score_val'].sum()}\")\n",
    "    print(f\"Sum of score_train: {user_data['score_train'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63497005-b360-4e0e-bf95-758af1fc1942",
   "metadata": {},
   "source": [
    "## Training Data Sparsity Analysis\n",
    "The heatmap visualization of user-track interactions reveals a critical limitation in our training data: users only interacted with a very narrow range of tracks (visible as sparse vertical green bands), while the vast majority of tracks had very low or zero interaction counts (dark purple regions).\n",
    "This extreme sparsity is particularly problematic because our model attempts to learn embeddings for all tracks present in tracks.json, regardless of their interaction count. During gradient descent optimization, we're trying to learn meaningful representations for thousands of tracks based on extremely limited interaction data. With such sparse information spread across such a large parameter space (embeddings for all tracks), the model struggles to learn meaningful patterns that would generalize well to recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd51783-7935-4363-ba98-856d7c60c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "heatmap_data = sessions_train_df.pivot_table(\n",
    "    values='score',\n",
    "    index='user_id',\n",
    "    columns='track_id',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.pcolormesh(heatmap_data, cmap='viridis')\n",
    "plt.colorbar(label='Score')\n",
    "plt.title('Heatmap of User-Track Scores')\n",
    "plt.xlabel('Track ID')\n",
    "plt.ylabel('User ID')\n",
    "\n",
    "plt.xticks(np.arange(0, len(heatmap_data.columns), 100), \n",
    "          heatmap_data.columns[::100], \n",
    "          rotation=45)\n",
    "plt.yticks(np.arange(0, len(heatmap_data.index), 100), \n",
    "          heatmap_data.index[::100])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200f735-196e-454a-b672-5f3d412df529",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Performance Analysis Conclusion\n",
    "\n",
    "During consulations, we identified two likely reasons for the model's poor performance:\n",
    "\n",
    "Our training dataset (~500MB) was several orders of magnitude smaller than what's typically needed for effective collaborative filtering (tens of gigabytes)\n",
    "The extreme sparsity of user-track interactions, where we attempted to learn embeddings for all tracks in tracks.json despite most tracks having very few interactions (many have zero interactions)\n",
    "\n",
    "These limitations likely prevented the model from learning meaningful user and track embeddings, explaining why most users had near-zero performance metrics except for a few cases where there was substantial overlap between training and validation interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdf7cb-e4fe-45a4-8b9e-a28452316a6c",
   "metadata": {},
   "source": [
    "# Comparison of Basic Model to advanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5922ce03-3b38-4004-b9de-630954649148",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_val_df[\"user_id\"] = user_encoder.decode(sessions_val_df[\"user_id\"].tolist())\n",
    "sessions_val_df[\"track_id\"] = track_encoder.decode(sessions_val_df[\"track_id\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829bce03-babf-42fe-8da8-80b02285cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.core.advanced_model import AdvancedModel\n",
    "advanced_model = AdvancedModel('model_files/wmf_model.pth')\n",
    "basic_model = BasicModel('data_files/train_sessions.jsonl')\n",
    "\n",
    "group_metrics_for_basic_model = evaluate_model_groups(\n",
    "    basic_model,\n",
    "    sessions_val_df,\n",
    "    n_groups=100,\n",
    "    group_size=5,\n",
    "    n_recommendations=[5, 10, 30]\n",
    ")\n",
    "\n",
    "group_metrics_for_advanced_model = evaluate_model_groups(\n",
    "    advanced_model,\n",
    "    sessions_val_df,\n",
    "    n_groups=100,\n",
    "    group_size=5,\n",
    "    n_recommendations=[5, 10, 30]\n",
    ")\n",
    "\n",
    "print_group_metrics(group_metrics_for_basic_model, [5, 10, 30])\n",
    "print_group_metrics(group_metrics_for_advanced_model, [5, 10, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c01e39-4d1b-4e61-9015-2f4b7a57ebaf",
   "metadata": {},
   "source": [
    "Looking at the performance metrics of both models, we can observe that the basic model slightly outperforms the advanced model.\n",
    "\n",
    "The basic model's better performance can be attributed to its deterministic nature - when a user in the validation set has listened to the same tracks as in the training set, the basic model will never make a mistake in recommendations. It simply recommends the tracks that the user has already shown interest in, which is a reliable strategy when historical patterns repeat.\n",
    "\n",
    "However, it's important to note that both models are primarily successful in cases where users in the validation set interact with the same tracks they previously interacted with in the training set. The basic model explicitly uses this historical interaction data, while the advanced model, despite its more sophisticated matrix factorization approach, is limited by the small training dataset and therefore also relies heavily on existing user-track interactions.\n",
    "\n",
    "If we were to train the advanced model on a significantly larger dataset (in the order of tens of gigabytes rather than the current ~500MB), we would expect to see much better performance compared to the basic model. This is because with more data, the matrix factorization model would be able to learn meaningful user and track embeddings, enabling it to identify patterns and similarities that go beyond simple historical interactions. This would allow the advanced model to make relevant recommendations even for tracks that users haven't previously interacted with, something the basic model fundamentally cannot do.\n",
    "\n",
    "In essence, while the basic model performs slightly better with the current limited dataset due to its deterministic nature with repeated interactions, the advanced model has the potential to significantly outperform the basic approach given sufficient training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
