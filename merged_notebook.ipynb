{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f616a39-4fe2-4d50-973b-3110e2c1723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ec365-6966-4442-afd6-6f0d27c666a5",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e309bd-056d-41d9-80ed-f5461d9bd545",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734718b9-3c51-42d8-947e-62a2212e4344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_cleaning import clean_sessions_jsonl\n",
    "\n",
    "clean_sessions_jsonl('data_files/tracks.jsonl', 'data_files/sessions.jsonl', 'data_files/sessions_clean.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e70505-353c-4b27-b121-76dad58c24e2",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c4fb9b-4f0d-48cc-b414-614f56aebc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_split import split_data\n",
    "\n",
    "sessions_df = pd.read_json('data_files/sessions_clean.jsonl', lines=True)\n",
    "train_data_df, val_data_df, test_data_df = split_data(sessions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f51636-536b-4396-9596-3d3c258e39a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Interaction Score Aggregation\n",
    "\n",
    "In this step, we transform raw user-track interaction events into aggregated scores that will be used for matrix factorization. The transformation process:\n",
    "\n",
    "1. Groups all events by unique user-track pairs\n",
    "2. Calculates a single interaction score for each pair based on event types:\n",
    "  - 'like' events contribute 1.0 to the score\n",
    "  - 'play' events contribute 1.0 to the score\n",
    "  - 'skip' events and other events contribute 0.0\n",
    "\n",
    "This converts sequential interaction logs (with timestamps and session IDs) into a format suitable for collaborative filtering - where each user-track pair has a single numerical score representing the user's overall interest level in that track. These scores will be used to train the matrix factorization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386f774a-920e-40d1-aed2-ed28c80d8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_transforming import aggregate_interactions\n",
    "\n",
    "train_data_aggregated_df = aggregate_interactions(train_data_df)\n",
    "val_data_aggregated_df = aggregate_interactions(val_data_df)\n",
    "test_data_aggregated_df = aggregate_interactions(test_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b4cf2-adaf-4434-bc2b-069fe49cb3fe",
   "metadata": {},
   "source": [
    "### Saving the datasets for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44335e43-a433-436d-a529-dd855027b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_aggregated_df.to_json('data_files/train_sessions.jsonl', orient='records', lines=True)\n",
    "val_data_aggregated_df.to_json('data_files/val_sessions.jsonl', orient='records', lines=True)\n",
    "test_data_aggregated_df.to_json('data_files/test_sessions.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b0b1a-3adb-456c-88cf-114eafdc38b5",
   "metadata": {},
   "source": [
    "### Loading the preprocessed training and validation session data from JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6355c369-dfe1-4141-95ec-4d6dd3550cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_train_df = pd.read_json('data_files/train_sessions.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e0e2c5b9-174f-4ec4-8907-4d502e6020e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_val_df = pd.read_json('data_files/val_sessions.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bac09-5800-4fd1-84ec-b80af74e74f6",
   "metadata": {},
   "source": [
    "# Basic model\n",
    "\n",
    "The basic model follows a straightforward approach to generate group recommendations: for each user in the group, it identifies their top 50 most played tracks based on historical listening data. It then combines these tracks across all group members and prioritizes songs that appear in multiple users' top tracks. When recommending N tracks, the algorithm first selects songs that are popular among the most group members, then fills any remaining slots randomly from the combined pool of top tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb439669-d886-465c-bff9-5567d64933f7",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98f41dd5-f0c2-49a7-abde-a87128088021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.core.basic_model import BasicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df5ca248-2f07-4d36-ac3e-3d4471d5f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = BasicModel(sessions_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6668a8-c5d9-4424-abc5-c3f36b9f0641",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c12ea-eed3-4720-a763-39a54316ae7a",
   "metadata": {},
   "source": [
    "1. Relevant Tracks:\n",
    "    - For a user group, get all tracks from validation set where:\n",
    "      * track was listened to by ANY user from the group\n",
    "      * interaction score >= 1.0\n",
    "\n",
    "2. Precision Calculation:\n",
    "    precision = (number of recommended tracks that appear in relevant tracks) / (total recommendations)\n",
    "    \n",
    "    Example: If 3 out of 10 recommended tracks were listened to by any group \n",
    "    member in validation set, precision = 0.3 (30%)\n",
    "\n",
    "3. Final Metrics:\n",
    "    - Calculate for different recommendation sizes (5, 10, 30 tracks)\n",
    "    - Take 10 random groups of 5 users each\n",
    "    - Report mean precision and standard deviation for each size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d995b094-d148-4bec-b731-43b7fbc9bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set, Tuple\n",
    "\n",
    "def get_group_relevant_tracks(val_df: pd.DataFrame, \n",
    "                            user_ids: List[int],\n",
    "                            relevance_threshold: float = 1.0) -> Set[int]:\n",
    "    relevant_interactions = val_df[\n",
    "        (val_df['user_id'].isin(user_ids)) & \n",
    "        (val_df['score'] >= relevance_threshold)\n",
    "    ]\n",
    "    return set(relevant_interactions['track_id'].unique())\n",
    "\n",
    "def calculate_group_precision(predicted_tracks: List[str],\n",
    "                          relevant_tracks: Set[int]) -> float:\n",
    "    predicted_set = set(predicted_tracks)\n",
    "    true_positives = len(relevant_tracks.intersection(predicted_set))\n",
    "    \n",
    "    return true_positives / len(predicted_set) if predicted_set else 0.0\n",
    "\n",
    "def evaluate_model_groups(model,\n",
    "                        val_df: pd.DataFrame,\n",
    "                        n_groups: int = 10,\n",
    "                        group_size: int = 5,\n",
    "                        n_recommendations: List[int] = [5, 10, 30],\n",
    "                        relevance_threshold: float = 1.0) -> Dict:\n",
    "    metrics = {n: [] for n in n_recommendations}\n",
    "    \n",
    "    all_users = val_df['user_id'].unique().tolist()\n",
    "    for _ in range(n_groups):\n",
    "        group_users = np.random.choice(all_users, size=group_size, replace=False)\n",
    "        \n",
    "        relevant_tracks = get_group_relevant_tracks(\n",
    "            val_df, group_users, relevance_threshold\n",
    "        )\n",
    "        \n",
    "        if not relevant_tracks:\n",
    "            continue\n",
    "            \n",
    "        for n in n_recommendations:\n",
    "            recs = model.get_recommendations(\n",
    "                [str(uid) for uid in group_users], \n",
    "                n\n",
    "            )\n",
    "            recs = [track_id for track_id in recs]\n",
    "            \n",
    "            precision = calculate_group_precision(recs, relevant_tracks)\n",
    "            metrics[n].append(precision)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_group_metrics(metrics: Dict, n_recommendations: List[int]) -> None:\n",
    "    print(\"\\nGroup Recommendations Performance:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for n in n_recommendations:\n",
    "        print(f\"\\nPrecision@{n}: {np.mean(metrics[n]):.3f} ± {np.std(metrics[n]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a63ecc-5aa4-433f-ba03-13a5863d79e1",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c56ffb6-4e40-4d41-b853-6175db8fe70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group Recommendations Performance:\n",
      "--------------------------------------------------\n",
      "\n",
      "Precision@5: 0.200 ± 0.126\n",
      "\n",
      "Precision@10: 0.200 ± 0.100\n",
      "\n",
      "Precision@30: 0.243 ± 0.110\n"
     ]
    }
   ],
   "source": [
    "basic_model = BasicModel(sessions_train_df)\n",
    "\n",
    "group_metrics_for_basic_model = evaluate_model_groups(\n",
    "    basic_model,\n",
    "    sessions_val_df,\n",
    "    n_groups=10,\n",
    "    group_size=5,\n",
    "    n_recommendations=[5, 10, 30]\n",
    ")\n",
    "\n",
    "print_group_metrics(group_metrics_for_basic_model, [5, 10, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c11c6e-9386-4fb0-83fd-20683f14c6ad",
   "metadata": {},
   "source": [
    "### Unique User-Track Combinations In Validation Set\n",
    "Our basic model can only recommend tracks users have already listened to, which is a significant limitation since 75.43% of user-track interactions in the validation set are completely new combinations not present in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c3fc562-0635-4374-b5e9-5185790f6d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.43% of user-track pairs in the validation set are not present in the training set\n"
     ]
    }
   ],
   "source": [
    "train_combinations = set(zip(sessions_train_df['user_id'], sessions_train_df['track_id']))\n",
    "val_combinations = set(zip(sessions_val_df['user_id'], sessions_val_df['track_id']))\n",
    "\n",
    "unique_to_val = val_combinations - train_combinations\n",
    "\n",
    "print(f\"{len(unique_to_val)/len(val_combinations)*100:.2f}% of user-track pairs in the validation set are not present in the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc81c52-4d1f-48b3-b4be-bf61e6cb01e6",
   "metadata": {},
   "source": [
    "## Basic Model Evaluation Summary\n",
    "Looking at the basic model's performance metrics, the results are surprisingly decent given that the model simply recommends tracks that group members have previously listened to, without any predictive capabilities.\n",
    "This is particularly interesting considering that 75.43% of user-track interactions in the validation set don't exist in the training data. The relatively good performance despite this limitation suggests that users tend to repeatedly listen to certain tracks over time, making historical listening data a useful signal even without sophisticated prediction mechanisms.\n",
    "From a business perspective, this simple approach has a key advantage: users are likely to be satisfied when their favorite tracks appear in group playlists. While more sophisticated models might discover new music more effectively, including well-loved tracks from users' listening history can help create a comfortable group listening experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e8686-9af8-4c52-988e-eac8f4dabc4f",
   "metadata": {},
   "source": [
    "# Advanced Model\n",
    "Advanced Model\n",
    "The advanced model implements collaborative filtering using matrix factorization (WMF - Weighted Matrix Factorization), which was thoroughly described in Phase 1 of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16eea5c-9756-49e4-a060-ceb5d86f02b6",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Before working with the WMF model, we need to properly encode user and track IDs. The model uses embedding layers which require continuous zero-based indices (0 to n-1) for lookup operations. Since our dataset contains non-sequential IDs (e.g., user_ids like 1001, 1578, 2403), we use `IDEncoder` object to map them to the required format. The encoders are saved alongside the model to ensure consistent ID mapping between training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c11fc-f433-45a7-aed7-db26fc11deaf",
   "metadata": {},
   "source": [
    "### Use when you would like to initialize new model ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e14b95f-1221-4429-84fc-d29ef5ddcac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.wmf import WMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82f250da-02b0-4385-81c5-e4b79c96ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.data_transforming import IDEncoder\n",
    "\n",
    "# users_df = pd.read_json('data_files/users.jsonl', lines=True)\n",
    "# tracks_df = pd.read_json('data_files/tracks.jsonl', lines=True)\n",
    "# user_encoder = IDEncoder()\n",
    "# track_encoder = IDEncoder()\n",
    "# user_encoder.fit(users_df[\"user_id\"].unique().tolist())\n",
    "# track_encoder.fit(tracks_df[\"id\"].unique().tolist())\n",
    "# model = WMF(\n",
    "#     n_users=len(user_encoder), \n",
    "#     n_items=len(track_encoder), \n",
    "#     embedding_dim=32, \n",
    "#     dropout_rate=0.0, \n",
    "#     init=True, \n",
    "#     bias=False, \n",
    "#     sigmoid=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e85d8-de36-43eb-9fc7-5ab524101896",
   "metadata": {},
   "source": [
    "### Use when you would like to load model from file ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2f817b6-eb27-4db4-9651-d64a02774b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = torch.load(\n",
    "    './model_files/wmf_model.pth',\n",
    "    map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "user_encoder = model_data[\"user_encoder\"]\n",
    "track_encoder = model_data[\"track_encoder\"]\n",
    "config = model_data[\"model_config\"]\n",
    "model = WMF(\n",
    "    n_users=len(user_encoder),\n",
    "    n_items=len(track_encoder),\n",
    "    embedding_dim=config[\"embedding_dim\"],\n",
    "    dropout_rate=config[\"dropout_rate\"],\n",
    "    bias=config[\"bias\"],\n",
    "    sigmoid=config[\"sigmoid\"],\n",
    "    init=False\n",
    ")\n",
    "model.load_state_dict(model_data[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ca268-bb70-4e2a-8182-3e7ab81a3446",
   "metadata": {},
   "source": [
    "### Using our previously created encoders to transform user and track IDs into continuous zero-based indices required by the embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c5f5cf27-6da8-4368-afcb-1d28bb13a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_train_encoded_df = sessions_train_df.copy()\n",
    "sessions_train_encoded_df = sessions_train_df.copy()\n",
    "sessions_val_encoded_df = sessions_val_df.copy()\n",
    "sessions_val_encoded_df = sessions_val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "51fa2ef1-891c-40fa-a9e1-53792c5977bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_train_encoded_df[\"user_id\"] = user_encoder.encode(sessions_train_df[\"user_id\"].tolist())\n",
    "sessions_train_encoded_df[\"track_id\"] = track_encoder.encode(sessions_train_df[\"track_id\"].tolist())\n",
    "sessions_val_encoded_df[\"user_id\"] = user_encoder.encode(sessions_val_df[\"user_id\"].tolist())\n",
    "sessions_val_encoded_df[\"track_id\"] = track_encoder.encode(sessions_val_df[\"track_id\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f458f32-e0c0-46ec-9ec4-99bd9b717984",
   "metadata": {},
   "source": [
    "### Generating Complete User-Track Interaction Matrix\n",
    "\n",
    "We create a complete interaction matrix using a subset of users (IDs 2000-2499) and all tracks. We take only a subset of users to reduce training time. First, we generate all possible user-track pairs using cartesian product, then merge this with our existing interaction data, filling missing scores with zeros (indicating no interaction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b9c516e-a2b9-44dd-9eed-8024f52fc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "users_range = np.arange(2000, 2500)\n",
    "tracks_range = np.arange(len(track_encoder))\n",
    "df = pd.DataFrame(\n",
    "    product(users_range, tracks_range),\n",
    "    columns=['user_id', 'track_id']\n",
    ")\n",
    "\n",
    "sessions_train_encoded_df = df.merge(\n",
    "    sessions_train_encoded_df[['user_id', 'track_id', 'score']],\n",
    "    on=['user_id', 'track_id'],\n",
    "    how='left'\n",
    ").fillna(0)\n",
    "\n",
    "sessions_val_encoded_df = df.merge(\n",
    "    sessions_val_encoded_df[['user_id', 'track_id', 'score']],\n",
    "    on=['user_id', 'track_id'], \n",
    "    how='left'\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d0f47-b73f-40f2-b298-46fb33a6fcc4",
   "metadata": {},
   "source": [
    "### Calculating Preferences and Confidence Weights\n",
    "\n",
    "Following the collaborative filtering algorithm, we transform each user-track interaction into two values:\n",
    "\n",
    "1. Preference:\n",
    "  - Set to 1 if score ≥ 1 (user interacted positively with track)\n",
    "  - Set to 0 if score = 0 (no interaction)\n",
    "\n",
    "2. Confidence Weight:\n",
    "  - Calculated using the formula: 1 + α * log(1 + score/ε)\n",
    "  - Where:\n",
    "    - α (alpha) = 40: hyperparameter controlling confidence scaling\n",
    "    - ε (epsilon) = 1e-8: small constant to prevent log(0)\n",
    "  - Higher interaction scores result in higher confidence values\n",
    "\n",
    "These values are then used in the model's objective function:\n",
    "\n",
    "$$\n",
    "\\min_{x,y} \\sum_{u,i} c_{ui}(p_{ui} - x_u^Ty_i)^2 + \\lambda(\\sum_u ||x_u||^2 + \\sum_i ||y_i||^2)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $c_{ui}$: confidence weight for interaction between user $u$ and track $i$\n",
    "- $p_{ui}$: preference value (0 or 1) for user $u$ and track $i$\n",
    "- $x_u$: embedding vector for user $u$\n",
    "- $y_i$: embedding vector for track $i$\n",
    "- $x_u^Ty_i$: dot product of user and track embeddings (predicted preference)\n",
    "- $\\lambda$: regularization parameter controlling the strength of regularization\n",
    "- $||x_u||^2$, $||y_i||^2$: L2 norms of embedding vectors (regularization terms)\n",
    "\n",
    "In our implementation, we use L2 regularization through the AdamW optimizer's weight decay parameter, which achieves the same effect of preventing overfitting by penalizing large embedding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c915386-c185-4927-8d2f-9ed1ec97f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_train_encoded_df['preference'] = sessions_train_encoded_df['score'] >= 1\n",
    "sessions_val_encoded_df['preference'] = sessions_val_encoded_df['score'] >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72688251-409f-46de-95e3-f81ac5a339f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 40\n",
    "epsilon = 1e-8\n",
    "sessions_train_encoded_df['weight'] = 1 + alpha * np.log(1 + sessions_train_encoded_df['score'] / epsilon)\n",
    "sessions_val_encoded_df['weight'] = 1 + alpha * np.log(1 + sessions_val_encoded_df['score'] / epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a297b08-a1e0-47a9-9181-3c1b94aa2287",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28cd2f5f-b277-482d-8954-37c2af386135",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.LongTensor(sessions_train_encoded_df['user_id'].values),\n",
    "    torch.LongTensor(sessions_train_encoded_df['track_id'].values),\n",
    "    torch.FloatTensor(sessions_train_encoded_df['preference'].values),\n",
    "    torch.FloatTensor(sessions_train_encoded_df['weight'].values)\n",
    ")\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.LongTensor(sessions_val_encoded_df['user_id'].values),\n",
    "    torch.LongTensor(sessions_val_encoded_df['track_id'].values),\n",
    "    torch.FloatTensor(sessions_val_encoded_df['preference'].values),\n",
    "    torch.FloatTensor(sessions_val_encoded_df['weight'].values)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46fc7882-cf69-49e9-a234-82ca20db6ecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, train_loader, val_loader, device)\n\u001b[0;32m----> 4\u001b[0m train_loss, val_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain_with_weight_decay(\n\u001b[1;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m      6\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m,\n\u001b[1;32m      7\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/uni_repos/IUM/models/training.py:44\u001b[0m, in \u001b[0;36mTrainer.train_with_weight_decay\u001b[0;34m(self, epochs, learning_rate, weight_decay)\u001b[0m\n\u001b[1;32m     41\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_ids, item_ids, labels, weights \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader:\n\u001b[1;32m     45\u001b[0m     user_ids \u001b[38;5;241m=\u001b[39m user_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     46\u001b[0m     item_ids \u001b[38;5;241m=\u001b[39m item_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.training import Trainer\n",
    "\n",
    "trainer = Trainer(model, train_loader, val_loader, device)\n",
    "train_loss, val_loss = trainer.train_with_weight_decay(\n",
    "    epochs=15,\n",
    "    learning_rate=0.005,\n",
    "    weight_decay=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e06199-d61b-4ad4-aff7-e36a0abdfd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, 'b-', label='Training Loss')\n",
    "plt.plot(val_loss, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095838fd-2244-497a-90aa-c81f0f52a7ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training Results Analysis\n",
    "\n",
    "The training process shows an interesting and unusual pattern where the validation loss is consistently lower than the training loss. After multiple code reviews and verifications of the implementation, we couldn't identify any bugs that would cause this behavior.\n",
    "\n",
    "This counterintuitive result might be explained by the data characteristics of our problem:\n",
    "1. We're training on all tracks from tracks.json, but users interacted with only a small subset of them (this will be explored later)\n",
    "2. This creates a highly sparse interaction matrix with many zero values\n",
    "3. Combined with:\n",
    "  - The large number of model parameters (embeddings for all users and tracks)\n",
    "  - Relatively small amount of actual interaction data\n",
    "  - The complexity of gradient descent optimization\n",
    "  \n",
    "This sparsity and parameter-to-data ratio could potentially lead to this unusual loss pattern. However, without further investigation, we cannot make a definitive conclusion about the cause of this behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ab17d-b032-4a7d-bf3b-0d009348cc85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Regularization Analysis\n",
    "\n",
    "To verify if our model's regularization is working correctly, we examined the norms of the learned embeddings:\n",
    "- Average user embedding norm: 0.34\n",
    "- Average item embedding norm: 0.98\n",
    "\n",
    "These values indicate that the regularization (implemented through AdamW's weight decay) is effectively preventing the embeddings from growing too large. This suggests that the unusual training/validation loss pattern is not caused by insufficient regularization, and we should look for other explanations in the data characteristics and model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ef7a080-a327-474f-b3f7-bf34cfdcfaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average user embedding norm: 0.3561004102230072\n",
      "Average item embedding norm: 0.9379550814628601\n"
     ]
    }
   ],
   "source": [
    "user_norms = torch.norm(model.user_embedding.weight, dim=1)\n",
    "avg_user_norm = torch.mean(user_norms) \n",
    "item_norms = torch.norm(model.item_embedding.weight, dim=1)\n",
    "avg_item_norm = torch.mean(item_norms)\n",
    "\n",
    "print(\"Average user embedding norm:\", avg_user_norm.item())\n",
    "print(\"Average item embedding norm:\", avg_item_norm.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f153c-627b-47f3-b5cb-69372ca5ae86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Saving the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5753b4aa-9d40-431f-bf38-9848fe93ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'embedding_dim': model.user_embedding.embedding_dim,\n",
    "        'dropout_rate': model.dropout.p,\n",
    "        'bias': model.bias,\n",
    "        'sigmoid': model.sigmoid,\n",
    "    },\n",
    "    'user_encoder': user_encoder,\n",
    "    'track_encoder': track_encoder\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, './model_files/wmf_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae7302-567a-4231-8c22-ea47efe3e4c6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8dfb26-ab75-465d-a5e1-5bd1a7670eca",
   "metadata": {},
   "source": [
    "### Use when you would like to load model from file ↓ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3417963f-98c4-47f1-8d50-9c146da33fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.wmf import WMF\n",
    "\n",
    "model_data = torch.load(\n",
    "    './model_files/wmf_model.pth',\n",
    "    map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "user_encoder = model_data[\"user_encoder\"]\n",
    "track_encoder = model_data[\"track_encoder\"]\n",
    "config = model_data[\"model_config\"]\n",
    "model = WMF(\n",
    "    n_users=len(user_encoder),\n",
    "    n_items=len(track_encoder),\n",
    "    embedding_dim=config[\"embedding_dim\"],\n",
    "    dropout_rate=config[\"dropout_rate\"],\n",
    "    bias=config[\"bias\"],\n",
    "    sigmoid=config[\"sigmoid\"],\n",
    "    init=False\n",
    ")\n",
    "model.load_state_dict(model_data[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dbe14b-469a-486b-a294-6f94b25b83ae",
   "metadata": {},
   "source": [
    "### Loading the validation data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c6dfd3c-1300-4a89-965b-a2fa89bbf8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_val_df = pd.read_json('data_files/val_sessions.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0082d-c709-4852-a02b-eee9925ab596",
   "metadata": {},
   "source": [
    "### Encode the validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1a0db959-f606-40f9-b334-6739dd9890e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_val_df[\"user_id\"] = user_encoder.encode(sessions_val_df[\"user_id\"].tolist())\n",
    "sessions_val_df[\"track_id\"] = track_encoder.encode(sessions_val_df[\"track_id\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155bf44a-ff03-4966-8727-86672c33d5ce",
   "metadata": {},
   "source": [
    "### Preparing Evaluation Data\n",
    "\n",
    "For evaluation purposes, we select a random subset of 50 users. For these users, we generate all possible user-track pairs to evaluate how well our model ranks all available tracks for each user.\n",
    "\n",
    "The process involves several steps:\n",
    "1. Select 50 random users from our user base using numpy's random choice function\n",
    "2. Create a cartesian product of these users with all available tracks in our dataset\n",
    "3. Merge this complete set of user-track pairs with our validation data (which contains actual user interactions)\n",
    "4. Fill missing values with 0 (representing no interaction)\n",
    "\n",
    "This approach assumes that the validation data (`sessions_val_df`) has already been aggregated using the `aggregate_interactions` function, which was done in the data preparation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "533dc57d-1b4e-4898-b644-b315757728ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ids = np.random.choice(len(user_encoder), size=50, replace=False)\n",
    "users = np.array(random_ids)\n",
    "tracks = np.arange(len(track_encoder))\n",
    "df = pd.DataFrame(\n",
    "    product(users, tracks),\n",
    "    columns=['user_id', 'track_id']\n",
    ")\n",
    "\n",
    "sessions_val_df = df.merge(\n",
    "    sessions_val_df[['user_id', 'track_id', 'score']],\n",
    "    on=['user_id', 'track_id'], \n",
    "    how='left'\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a446f23-bee3-4608-8f91-df9d772a5b83",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "140dce07-0aa9-473f-ae15-f39ffc0e1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.LongTensor(sessions_val_encoded_df['user_id'].values),\n",
    "    torch.LongTensor(sessions_val_encoded_df['track_id'].values),\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "100cb7a2-8dcc-4796-a9a1-406d7aed3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.predict import predict\n",
    "sessions_val_encoded_df['score_pred'] = predict(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e836ee7c-0b5a-4c70-948b-597de1c21c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [5, 10, 30]\n",
    "\n",
    "from evaluation.evaluator import Evaluator\n",
    "evaluator = Evaluator(k_list, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "24749618-bee7-43cf-b1dd-927d6c6c64c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Computing NDCG is only meaningful when there is more than 1 document. Got 1 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(sessions_val_encoded_df)\n",
      "File \u001b[0;32m~/uni_repos/IUM/evaluation/evaluator.py:14\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, pred_sessions_df)\u001b[0m\n\u001b[1;32m     12\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_values:\n\u001b[0;32m---> 14\u001b[0m     ndcg_scores \u001b[38;5;241m=\u001b[39m pred_sessions_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: calculate_ndcg(x, k)   \n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     18\u001b[0m     recall_scores \u001b[38;5;241m=\u001b[39m pred_sessions_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: calculate_recall(x, k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevance_threshold)\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     precision_scores \u001b[38;5;241m=\u001b[39m pred_sessions_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: calculate_precision(x, k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevance_threshold)\n\u001b[1;32m     24\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[0;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[1;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[1;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1829\u001b[0m         ):\n\u001b[1;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1836\u001b[0m             )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[1;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[0;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[1;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/uni_repos/IUM/evaluation/evaluator.py:15\u001b[0m, in \u001b[0;36mEvaluator.evaluate.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_values:\n\u001b[1;32m     14\u001b[0m     ndcg_scores \u001b[38;5;241m=\u001b[39m pred_sessions_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m---> 15\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: calculate_ndcg(x, k)   \n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     18\u001b[0m     recall_scores \u001b[38;5;241m=\u001b[39m pred_sessions_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: calculate_recall(x, k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevance_threshold)\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     precision_scores \u001b[38;5;241m=\u001b[39m pred_sessions_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: calculate_precision(x, k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevance_threshold)\n\u001b[1;32m     24\u001b[0m     )\n",
      "File \u001b[0;32m~/uni_repos/IUM/evaluation/metrics.py:22\u001b[0m, in \u001b[0;36mcalculate_ndcg\u001b[0;34m(user_df, k)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     gt_normalized \u001b[38;5;241m=\u001b[39m (gt_scores \u001b[38;5;241m-\u001b[39m gt_scores\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m gt_range\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ndcg_score(y_true\u001b[38;5;241m=\u001b[39mgt_normalized\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     23\u001b[0m                  y_score\u001b[38;5;241m=\u001b[39mpred_normalized\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     24\u001b[0m                  k\u001b[38;5;241m=\u001b[39mk)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:1834\u001b[0m, in \u001b[0;36mndcg_score\u001b[0;34m(y_true, y_score, k, sample_weight, ignore_ties)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndcg_score should not be used on negative y_true values.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1834\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1835\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing NDCG is only meaningful when there is more than 1 document. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1836\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1837\u001b[0m     )\n\u001b[1;32m   1838\u001b[0m _check_dcg_target_type(y_true)\n\u001b[1;32m   1839\u001b[0m gain \u001b[38;5;241m=\u001b[39m _ndcg_sample_scores(y_true, y_score, k\u001b[38;5;241m=\u001b[39mk, ignore_ties\u001b[38;5;241m=\u001b[39mignore_ties)\n",
      "\u001b[0;31mValueError\u001b[0m: Computing NDCG is only meaningful when there is more than 1 document. Got 1 instead."
     ]
    }
   ],
   "source": [
    "metrics = evaluator.evaluate(sessions_val_encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ae9e1dde-5593-4e9f-af5f-e056b6cf46ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id  track_id  score    score_pred\n",
      "0             0       154      2  8.294075e-09\n",
      "1             0       465      2  8.561018e-09\n",
      "2             0       618      1  5.953964e-09\n",
      "3             0       625      1  4.037841e-09\n",
      "4             0       735      2 -5.029880e-09\n",
      "...         ...       ...    ...           ...\n",
      "357306     2999     21778      1 -3.387209e-11\n",
      "357307     2999     21832      2  1.964314e-14\n",
      "357308     2999     22124      2 -1.761560e-11\n",
      "357309     2999     22167      2 -9.065224e-12\n",
      "357310     2999     22180      1 -6.968835e-11\n",
      "\n",
      "[357311 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sessions_val_encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad63a1-43de-4442-af53-3fb7a075599f",
   "metadata": {},
   "source": [
    "## Overall Model Performance\n",
    "Looking at the mean metric values, we can see that the model performs only slightly better than random recommendations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "734ee510-487e-45eb-8f53-208e851d3ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics:\n",
      "==================================================\n",
      "\n",
      "Metrics for top-5 recommendations:\n",
      "-----------------------------------\n",
      "NDCG       =   3.47% ±   8.06%\n",
      "Precision  =   2.40% ±   9.60%\n",
      "Recall     =   0.84% ±   4.76%\n",
      "\n",
      "Metrics for top-10 recommendations:\n",
      "-----------------------------------\n",
      "NDCG       =   3.90% ±   8.35%\n",
      "Precision  =   3.40% ±  12.22%\n",
      "Recall     =   1.28% ±   5.12%\n",
      "\n",
      "Metrics for top-30 recommendations:\n",
      "-----------------------------------\n",
      "NDCG       =   4.52% ±   9.21%\n",
      "Precision  =   3.07% ±  10.36%\n",
      "Recall     =   3.52% ±   9.35%\n"
     ]
    }
   ],
   "source": [
    "from evaluation.metrics import print_metrics\n",
    "print_metrics(metrics, k_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8cae4-a935-4abe-b7bf-8987c5a6c09b",
   "metadata": {},
   "source": [
    "## Per user metrics display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d575a3b3-4990-427e-ad17-d30be64060c9",
   "metadata": {},
   "source": [
    "To investigate model performance, we displayed metrics for each individual user. The results showed that most users had metrics equal to zero, while a few users (e.g., user 979 with NDCG@5 = 0.619 and Precision@5 = 1.0) showed exceptionally high performance. This interesting pattern led us to investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0942f1b1-54f9-4058-b1cf-30243267fbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>NDCG@30</th>\n",
       "      <th>Precision@30</th>\n",
       "      <th>Recall@30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.126632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.101005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082590</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.260374</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.307082</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.276749</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.135135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>0.070489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>0.189179</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.276670</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.330228</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>0.101005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>0.028816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094745</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.165285</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>0.182211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>0.365943</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.388804</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.500871</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>0.248427</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.189057</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.111743</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>0.062445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2580</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009879</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           NDCG@5  Precision@5  Recall@5   NDCG@10  Precision@10  Recall@10  \\\n",
       "user_id                                                                       \n",
       "36       0.126632          0.0  0.000000  0.090195           0.0   0.000000   \n",
       "151      0.000000          0.0  0.000000  0.048381           0.0   0.000000   \n",
       "155      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "243      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "268      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "381      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "428      0.101005          0.0  0.000000  0.073507           0.0   0.000000   \n",
       "451      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "469      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "483      0.260374          0.2  0.013514  0.307082           0.4   0.054054   \n",
       "485      0.000000          0.0  0.000000  0.041245           0.0   0.000000   \n",
       "585      0.070489          0.0  0.000000  0.050997           0.0   0.000000   \n",
       "725      0.000000          0.0  0.000000  0.047955           0.0   0.000000   \n",
       "745      0.189179          0.2  0.015152  0.276670           0.5   0.075758   \n",
       "764      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "822      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "859      0.101005          0.0  0.000000  0.073507           0.0   0.000000   \n",
       "875      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "891      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "909      0.028816          0.0  0.000000  0.094745           0.1   0.066667   \n",
       "921      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "980      0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1027     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1151     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1270     0.000000          0.0  0.000000  0.040684           0.0   0.000000   \n",
       "1289     0.000000          0.0  0.000000  0.057314           0.0   0.000000   \n",
       "1314     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1322     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1375     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1430     0.182211          0.0  0.000000  0.129261           0.0   0.000000   \n",
       "1505     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1544     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1671     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1789     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1888     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1894     0.365943          0.6  0.055556  0.388804           0.6   0.111111   \n",
       "1990     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "1994     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2125     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2134     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2342     0.248427          0.2  0.333333  0.189057           0.1   0.333333   \n",
       "2358     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2443     0.062445          0.0  0.000000  0.042696           0.0   0.000000   \n",
       "2458     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2580     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2607     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2611     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2718     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2720     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "2962     0.000000          0.0  0.000000  0.000000           0.0   0.000000   \n",
       "\n",
       "          NDCG@30  Precision@30  Recall@30  \n",
       "user_id                                     \n",
       "36       0.049205      0.000000   0.000000  \n",
       "151      0.028966      0.000000   0.000000  \n",
       "155      0.026176      0.000000   0.000000  \n",
       "243      0.020055      0.000000   0.000000  \n",
       "268      0.000000      0.000000   0.000000  \n",
       "381      0.012865      0.000000   0.000000  \n",
       "428      0.041107      0.000000   0.000000  \n",
       "451      0.000000      0.000000   0.000000  \n",
       "469      0.082590      0.066667   0.400000  \n",
       "483      0.276749      0.333333   0.135135  \n",
       "485      0.025261      0.000000   0.000000  \n",
       "585      0.095121      0.000000   0.000000  \n",
       "725      0.029570      0.000000   0.000000  \n",
       "745      0.330228      0.400000   0.181818  \n",
       "764      0.000000      0.000000   0.000000  \n",
       "822      0.000000      0.000000   0.000000  \n",
       "859      0.041107      0.000000   0.000000  \n",
       "875      0.000000      0.000000   0.000000  \n",
       "891      0.000000      0.000000   0.000000  \n",
       "909      0.165285      0.100000   0.200000  \n",
       "921      0.028133      0.000000   0.000000  \n",
       "980      0.000000      0.000000   0.000000  \n",
       "1027     0.000000      0.000000   0.000000  \n",
       "1151     0.036036      0.033333   0.200000  \n",
       "1270     0.024658      0.000000   0.000000  \n",
       "1289     0.029919      0.000000   0.000000  \n",
       "1314     0.019586      0.000000   0.000000  \n",
       "1322     0.000000      0.000000   0.000000  \n",
       "1375     0.000000      0.000000   0.000000  \n",
       "1430     0.080944      0.000000   0.000000  \n",
       "1505     0.000000      0.000000   0.000000  \n",
       "1544     0.034739      0.000000   0.000000  \n",
       "1671     0.034313      0.000000   0.000000  \n",
       "1789     0.000000      0.000000   0.000000  \n",
       "1888     0.000000      0.000000   0.000000  \n",
       "1894     0.500871      0.533333   0.296296  \n",
       "1990     0.013867      0.000000   0.000000  \n",
       "1994     0.000000      0.000000   0.000000  \n",
       "2125     0.032631      0.000000   0.000000  \n",
       "2134     0.000000      0.000000   0.000000  \n",
       "2342     0.111743      0.033333   0.333333  \n",
       "2358     0.028872      0.000000   0.000000  \n",
       "2443     0.022288      0.000000   0.000000  \n",
       "2458     0.000000      0.000000   0.000000  \n",
       "2580     0.029586      0.000000   0.000000  \n",
       "2607     0.000000      0.000000   0.000000  \n",
       "2611     0.000000      0.000000   0.000000  \n",
       "2718     0.000000      0.000000   0.000000  \n",
       "2720     0.000000      0.000000   0.000000  \n",
       "2962     0.009879      0.033333   0.011494  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame()\n",
    "for k in k_list:\n",
    "    metrics_df[f'NDCG@{k}'] = metrics[f'NDCG@{k}']\n",
    "    metrics_df[f'Precision@{k}'] = metrics[f'Precision@{k}']\n",
    "    metrics_df[f'Recall@{k}'] = metrics[f'Recall@{k}']\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36f456-557b-4676-8239-fbd558461278",
   "metadata": {},
   "source": [
    "### Analysis of Model Performance Variations\n",
    "We compared two users - one with high metrics (user_id 2342, NDCG@5 = 0.619) and one with near-zero metrics (user_id 2607, NDCG@5 = 0) - to understand why only a few users had high metric scores. Our analysis revealed that for users with high metrics, the tracks they listened to in the validation set were also present in their training set interactions. For users with near-zero metrics, there was little to no overlap between their training and validation tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "40bff8ed-ce3e-451d-9ca1-af2b740ca0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User 2342:\n",
      "Sum of score_val: 295\n",
      "Sum of score_train: 37.0\n",
      "\n",
      "User 2458:\n",
      "Sum of score_val: 118\n",
      "Sum of score_train: 5.0\n"
     ]
    }
   ],
   "source": [
    "merged_df = sessions_val_encoded_df.merge(\n",
    "    sessions_train_encoded_df, \n",
    "    on=['user_id', 'track_id'], \n",
    "    how='left',\n",
    "    suffixes=('_val', '_train')\n",
    ")\n",
    "\n",
    "for user_id in [2342, 2458]:\n",
    "    print(f\"\\nUser {user_id}:\")\n",
    "    user_data = merged_df[merged_df['user_id'] == user_id]\n",
    "    print(f\"Sum of score_val: {user_data['score_val'].sum()}\")\n",
    "    print(f\"Sum of score_train: {user_data['score_train'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63497005-b360-4e0e-bf95-758af1fc1942",
   "metadata": {},
   "source": [
    "## Training Data Sparsity Analysis\n",
    "The heatmap visualization of user-track interactions reveals a critical limitation in our training data: users only interacted with a very narrow range of tracks (visible as sparse vertical green bands), while the vast majority of tracks had very low or zero interaction counts (dark purple regions).\n",
    "This extreme sparsity is particularly problematic because our model attempts to learn embeddings for all tracks present in tracks.json, regardless of their interaction count. During gradient descent optimization, we're trying to learn meaningful representations for thousands of tracks based on extremely limited interaction data. With such sparse information spread across such a large parameter space (embeddings for all tracks), the model struggles to learn meaningful patterns that would generalize well to recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd51783-7935-4363-ba98-856d7c60c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "heatmap_data = sessions_train_df.pivot_table(\n",
    "    values='score',\n",
    "    index='user_id',\n",
    "    columns='track_id',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.pcolormesh(heatmap_data, cmap='viridis')\n",
    "plt.colorbar(label='Score')\n",
    "plt.title('Heatmap of User-Track Scores')\n",
    "plt.xlabel('Track ID')\n",
    "plt.ylabel('User ID')\n",
    "\n",
    "plt.xticks(np.arange(0, len(heatmap_data.columns), 100), \n",
    "          heatmap_data.columns[::100], \n",
    "          rotation=45)\n",
    "plt.yticks(np.arange(0, len(heatmap_data.index), 100), \n",
    "          heatmap_data.index[::100])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200f735-196e-454a-b672-5f3d412df529",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Performance Analysis Conclusion\n",
    "\n",
    "During consulations, we identified two likely reasons for the model's poor performance:\n",
    "\n",
    "Our training dataset (~500MB) was several orders of magnitude smaller than what's typically needed for effective collaborative filtering (tens of gigabytes)\n",
    "The extreme sparsity of user-track interactions, where we attempted to learn embeddings for all tracks in tracks.json despite most tracks having very few interactions (many have zero interactions)\n",
    "\n",
    "These limitations likely prevented the model from learning meaningful user and track embeddings, explaining why most users had near-zero performance metrics except for a few cases where there was substantial overlap between training and validation interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdf7cb-e4fe-45a4-8b9e-a28452316a6c",
   "metadata": {},
   "source": [
    "# Comparison of Basic Model to advanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "829bce03-babf-42fe-8da8-80b02285cdac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: [36]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m basic_model \u001b[38;5;241m=\u001b[39m BasicModel(sessions_train_df)\n\u001b[1;32m      5\u001b[0m group_metrics_for_basic_model \u001b[38;5;241m=\u001b[39m evaluate_model_groups(\n\u001b[1;32m      6\u001b[0m     basic_model,\n\u001b[1;32m      7\u001b[0m     sessions_val_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     n_recommendations\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m]\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m group_metrics_for_advanced_model \u001b[38;5;241m=\u001b[39m evaluate_model_groups(\n\u001b[1;32m     14\u001b[0m     advanced_model,\n\u001b[1;32m     15\u001b[0m     sessions_val_df,\n\u001b[1;32m     16\u001b[0m     n_groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     17\u001b[0m     group_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     18\u001b[0m     n_recommendations\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m]\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m print_group_metrics(group_metrics_for_basic_model, [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m])\n\u001b[1;32m     22\u001b[0m print_group_metrics(group_metrics_for_advanced_model, [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m])\n",
      "Cell \u001b[0;32mIn[41], line 39\u001b[0m, in \u001b[0;36mevaluate_model_groups\u001b[0;34m(model, val_df, n_groups, group_size, n_recommendations, relevance_threshold)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m n_recommendations:\n\u001b[0;32m---> 39\u001b[0m     recs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_recommendations(\n\u001b[1;32m     40\u001b[0m         [\u001b[38;5;28mstr\u001b[39m(uid) \u001b[38;5;28;01mfor\u001b[39;00m uid \u001b[38;5;129;01min\u001b[39;00m group_users], \n\u001b[1;32m     41\u001b[0m         n\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m     recs \u001b[38;5;241m=\u001b[39m [track_id \u001b[38;5;28;01mfor\u001b[39;00m track_id \u001b[38;5;129;01min\u001b[39;00m recs]\n\u001b[1;32m     45\u001b[0m     precision \u001b[38;5;241m=\u001b[39m calculate_group_precision(recs, relevant_tracks)\n",
      "File \u001b[0;32m~/uni_repos/IUM/service/core/advanced_model.py:35\u001b[0m, in \u001b[0;36mAdvancedModel.get_recommendations\u001b[0;34m(self, user_ids, n_recommendations)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_recommendations\u001b[39m(\u001b[38;5;28mself\u001b[39m, user_ids: List[\u001b[38;5;28mstr\u001b[39m], n_recommendations: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 35\u001b[0m     users \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_encoder\u001b[38;5;241m.\u001b[39mencode(user_ids))\n\u001b[1;32m     36\u001b[0m     tracks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_tracks_count)\n\u001b[1;32m     37\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m     38\u001b[0m             product(users, tracks),\n\u001b[1;32m     39\u001b[0m             columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     40\u001b[0m             )\n",
      "File \u001b[0;32m~/uni_repos/IUM/data/data_transforming.py:19\u001b[0m, in \u001b[0;36mIDEncoder.encode\u001b[0;34m(self, hash_ids)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_fitted:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoder must be trained first using method fit()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m numeric_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_encoder\u001b[38;5;241m.\u001b[39mtransform(hash_ids)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m numeric_ids\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/_encode.py:232\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    230\u001b[0m     diff \u001b[38;5;241m=\u001b[39m _check_unknown(values, uniques)\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[0;32m--> 232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(diff)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msearchsorted(uniques, values)\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: [36]"
     ]
    }
   ],
   "source": [
    "from service.core.advanced_model import AdvancedModel\n",
    "advanced_model = AdvancedModel('model_files/wmf_model.pth')\n",
    "basic_model = BasicModel(sessions_train_df)\n",
    "\n",
    "group_metrics_for_basic_model = evaluate_model_groups(\n",
    "    basic_model,\n",
    "    sessions_val_df,\n",
    "    n_groups=100,\n",
    "    group_size=5,\n",
    "    n_recommendations=[5, 10, 30]\n",
    ")\n",
    "\n",
    "group_metrics_for_advanced_model = evaluate_model_groups(\n",
    "    advanced_model,\n",
    "    sessions_val_df,\n",
    "    n_groups=100,\n",
    "    group_size=5,\n",
    "    n_recommendations=[5, 10, 30]\n",
    ")\n",
    "\n",
    "print_group_metrics(group_metrics_for_basic_model, [5, 10, 30])\n",
    "print_group_metrics(group_metrics_for_advanced_model, [5, 10, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c01e39-4d1b-4e61-9015-2f4b7a57ebaf",
   "metadata": {},
   "source": [
    "Looking at the performance metrics of both models, we can observe that the basic model slightly outperforms the advanced model.\n",
    "\n",
    "The basic model's better performance can be attributed to its deterministic nature - when a user in the validation set has listened to the same tracks as in the training set, the basic model will never make a mistake in recommendations. It simply recommends the tracks that the user has already shown interest in, which is a reliable strategy when historical patterns repeat.\n",
    "\n",
    "However, it's important to note that both models are primarily successful in cases where users in the validation set interact with the same tracks they previously interacted with in the training set. The basic model explicitly uses this historical interaction data, while the advanced model, despite its more sophisticated matrix factorization approach, is limited by the small training dataset and therefore also relies heavily on existing user-track interactions.\n",
    "\n",
    "If we were to train the advanced model on a significantly larger dataset (in the order of tens of gigabytes rather than the current ~500MB), we would expect to see much better performance compared to the basic model. This is because with more data, the matrix factorization model would be able to learn meaningful user and track embeddings, enabling it to identify patterns and similarities that go beyond simple historical interactions. This would allow the advanced model to make relevant recommendations even for tracks that users haven't previously interacted with, something the basic model fundamentally cannot do.\n",
    "\n",
    "In essence, while the basic model performs slightly better with the current limited dataset due to its deterministic nature with repeated interactions, the advanced model has the potential to significantly outperform the basic approach given sufficient training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
