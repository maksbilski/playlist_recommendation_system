{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f616a39-4fe2-4d50-973b-3110e2c1723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.training import Trainer\n",
    "from models.wmf import WMF\n",
    "from data.data_transforming import IDEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ec365-6966-4442-afd6-6f0d27c666a5",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e309bd-056d-41d9-80ed-f5461d9bd545",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734718b9-3c51-42d8-947e-62a2212e4344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_cleaning import clean_sessions_jsonl\n",
    "\n",
    "clean_sessions_jsonl('data_files/tracks.jsonl', 'data_files/sessions.jsonl', 'data_files/sessions_clean.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e70505-353c-4b27-b121-76dad58c24e2",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4fb9b-4f0d-48cc-b414-614f56aebc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_split import split_data\n",
    "\n",
    "sessions_df = pd.read_json('data_files/sessions_clean.jsonl', lines=True)\n",
    "train_data_df, val_data_df, test_data_df = split_data(sessions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f51636-536b-4396-9596-3d3c258e39a5",
   "metadata": {},
   "source": [
    "### Interaction Score Aggregation\n",
    "\n",
    "In this step, we transform raw user-track interaction events into aggregated scores that will be used for matrix factorization. The transformation process:\n",
    "\n",
    "1. Groups all events by unique user-track pairs\n",
    "2. Calculates a single interaction score for each pair based on event types:\n",
    "  - 'like' events contribute 1.0 to the score\n",
    "  - 'play' events contribute 1.0 to the score\n",
    "  - 'skip' events and other events contribute 0.0\n",
    "\n",
    "This converts sequential interaction logs (with timestamps and session IDs) into a format suitable for collaborative filtering - where each user-track pair has a single numerical score representing the user's overall interest level in that track. These scores will be used to train the matrix factorization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f774a-920e-40d1-aed2-ed28c80d8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_transforming import aggregate_interactions\n",
    "\n",
    "train_data_aggregated_df = aggregate_interactions(train_data_df)\n",
    "val_data_aggregated_df = aggregate_interactions(val_data_df)\n",
    "test_data_aggregated_df = aggregate_interactions(test_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b4cf2-adaf-4434-bc2b-069fe49cb3fe",
   "metadata": {},
   "source": [
    "### Saving the datasets for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44335e43-a433-436d-a529-dd855027b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_aggregated_df.to_json('data_files/train_sessions.jsonl', orient='records', lines=True)\n",
    "val_data_aggregated_df.to_json('data_files/val_sessions.jsonl', orient='records', lines=True)\n",
    "test_data_aggregated_df.to_json('data_files/test_sessions.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b0b1a-3adb-456c-88cf-114eafdc38b5",
   "metadata": {},
   "source": [
    "### Loading the preprocessed training and validation session data from JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6355c369-dfe1-4141-95ec-4d6dd3550cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_train_df = pd.read_json('data_files/train_sessions.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0e2c5b9-174f-4ec4-8907-4d502e6020e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_val_df = pd.read_json('data_files/val_sessions.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa799b78-4e81-411f-bdf1-6f897ec84806",
   "metadata": {},
   "source": [
    "# Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb439669-d886-465c-bff9-5567d64933f7",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98f41dd5-f0c2-49a7-abde-a87128088021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.core.basic_model import BasicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df5ca248-2f07-4d36-ac3e-3d4471d5f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = BasicModel(sessions_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6668a8-c5d9-4424-abc5-c3f36b9f0641",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d995b094-d148-4bec-b731-43b7fbc9bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Set, Tuple\n",
    "\n",
    "def get_group_relevant_tracks(val_df: pd.DataFrame, \n",
    "                            user_ids: List[int],\n",
    "                            relevance_threshold: float = 1.0) -> Set[int]:\n",
    "    relevant_interactions = val_df[\n",
    "        (val_df['user_id'].isin(user_ids)) & \n",
    "        (val_df['score'] >= relevance_threshold)\n",
    "    ]\n",
    "    return set(relevant_interactions['track_id'].unique())\n",
    "\n",
    "def calculate_group_precision(predicted_tracks: List[str],\n",
    "                          relevant_tracks: Set[int]) -> float:\n",
    "    predicted_set = set(predicted_tracks)\n",
    "    true_positives = len(relevant_tracks.intersection(predicted_set))\n",
    "    \n",
    "    return true_positives / len(predicted_set) if predicted_set else 0.0\n",
    "\n",
    "def evaluate_model_groups(model,\n",
    "                        val_df: pd.DataFrame,\n",
    "                        n_groups: int = 10,\n",
    "                        group_size: int = 5,\n",
    "                        n_recommendations: List[int] = [5, 10, 30],\n",
    "                        relevance_threshold: float = 1.0) -> Dict:\n",
    "    metrics = {n: [] for n in n_recommendations}\n",
    "    \n",
    "    all_users = val_df['user_id'].unique().tolist()\n",
    "    for _ in range(n_groups):\n",
    "        group_users = np.random.choice(all_users, size=group_size, replace=False)\n",
    "        \n",
    "        relevant_tracks = get_group_relevant_tracks(\n",
    "            val_df, group_users, relevance_threshold\n",
    "        )\n",
    "        \n",
    "        if not relevant_tracks:\n",
    "            continue\n",
    "            \n",
    "        for n in n_recommendations:\n",
    "            recs = model.get_recommendations(\n",
    "                [str(uid) for uid in group_users], \n",
    "                n\n",
    "            )\n",
    "            recs = [track_id for track_id in recs]\n",
    "            \n",
    "            precision = calculate_group_precision(recs, relevant_tracks)\n",
    "            metrics[n].append(precision)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_group_metrics(metrics: Dict, n_recommendations: List[int]) -> None:\n",
    "    print(\"\\nGroup Recommendations Performance:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for n in n_recommendations:\n",
    "        print(f\"\\nPrecision@{n}: {np.mean(metrics[n]):.3f} ± {np.std(metrics[n]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c56ffb6-4e40-4d41-b853-6175db8fe70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group Recommendations Performance:\n",
      "--------------------------------------------------\n",
      "\n",
      "Precision@5: 0.280 ± 0.256\n",
      "\n",
      "Precision@10: 0.341 ± 0.290\n",
      "\n",
      "Precision@30: 0.304 ± 0.214\n"
     ]
    }
   ],
   "source": [
    "basic_model = BasicModel(sessions_train_df)\n",
    "\n",
    "group_metrics_for_basic_model = evaluate_model_groups(\n",
    "    basic_model,\n",
    "    sessions_val_df,\n",
    "    n_groups=10,\n",
    "    group_size=5,\n",
    "    n_recommendations=[5, 10, 30]\n",
    ")\n",
    "\n",
    "print_group_metrics(group_metrics_for_basic_model, [5, 10, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fc562-0635-4374-b5e9-5185790f6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stwórz zbiory kombinacji (user_id, track_id)\n",
    "train_combinations = set(zip(train_df['user_id'], train_df['track_id']))\n",
    "val_combinations = set(zip(val_df['user_id'], val_df['track_id']))\n",
    "\n",
    "# Policz unikalne kombinacje\n",
    "unique_to_val = val_combinations - train_combinations\n",
    "unique_to_train = train_combinations - val_combinations\n",
    "common_combinations = train_combinations & val_combinations\n",
    "\n",
    "\n",
    "unique_to_val = val_combinations - train_combinations\n",
    "\n",
    "print(f\"Training set combinations: {len(train_combinations)}\")\n",
    "print(f\"Validation set combinations: {len(val_combinations)}\")\n",
    "print(f\"Combinations unique to validation set: {len(unique_to_val)}\")\n",
    "print(269511/357311)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e8686-9af8-4c52-988e-eac8f4dabc4f",
   "metadata": {},
   "source": [
    "# Advanced Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16eea5c-9756-49e4-a060-ceb5d86f02b6",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Before working with the WMF model, we need to properly encode user and track IDs. The model uses embedding layers which require continuous zero-based indices (0 to n-1) for lookup operations. Since our dataset contains non-sequential IDs (e.g., user_ids like 1001, 1578, 2403), we use `IDEncoder` object to map them to the required format. The encoders are saved alongside the model to ensure consistent ID mapping between training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c11fc-f433-45a7-aed7-db26fc11deaf",
   "metadata": {},
   "source": [
    "### Use when you would like to initialize new model ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f250da-02b0-4385-81c5-e4b79c96ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_json('data_files/users.jsonl', lines=True)\n",
    "tracks_df = pd.read_json('data_files/tracks.jsonl', lines=True)\n",
    "user_encoder = IDEncoder()\n",
    "track_encoder = IDEncoder()\n",
    "user_encoder.fit(users_df[\"user_id\"].unique().tolist())\n",
    "track_encoder.fit(tracks_df[\"id\"].unique().tolist())\n",
    "model = WMF(\n",
    "    n_users=len(user_encoder), \n",
    "    n_items=len(track_encoder), \n",
    "    embedding_dim=32, \n",
    "    dropout_rate=0.0, \n",
    "    init=True, \n",
    "    bias=False, \n",
    "    sigmoid=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e85d8-de36-43eb-9fc7-5ab524101896",
   "metadata": {},
   "source": [
    "### Use when you would like to load model from file ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f817b6-eb27-4db4-9651-d64a02774b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = torch.load(\n",
    "    './model_files/wmf_model.pth',\n",
    "    map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "user_encoder = model_data[\"user_encoder\"]\n",
    "track_encoder = model_data[\"track_encoder\"]\n",
    "config = model_data[\"model_config\"]\n",
    "model = WMF(\n",
    "    n_users=len(user_encoder),\n",
    "    n_items=len(track_encoder),\n",
    "    embedding_dim=config[\"embedding_dim\"],\n",
    "    dropout_rate=config[\"dropout_rate\"],\n",
    "    bias=config[\"bias\"],\n",
    "    sigmoid=config[\"sigmoid\"],\n",
    "    init=False\n",
    ")\n",
    "model.load_state_dict(model_data[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aaef58-b607-4a49-92ae-9ed62d97326c",
   "metadata": {},
   "source": [
    "### Using our previously created encoders to transform user and track IDs into continuous zero-based indices required by the embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802cd34f-45d5-4ed3-8cae-c57141534a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f458f32-e0c0-46ec-9ec4-99bd9b717984",
   "metadata": {},
   "source": [
    "### Generating Complete User-Track Interaction Matrix\n",
    "\n",
    "We create a complete interaction matrix using a subset of users (IDs 2000-2499) and all tracks. We take only a subset of users to reduce training time. First, we generate all possible user-track pairs using cartesian product, then merge this with our existing interaction data, filling missing scores with zeros (indicating no interaction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9c516e-a2b9-44dd-9eed-8024f52fc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_range = np.arange(2000, 2500)\n",
    "tracks_range = np.arange(len(track_encoder))\n",
    "df = pd.DataFrame(\n",
    "    product(users_range, tracks_range),\n",
    "    columns=['user_id', 'track_id']\n",
    ")\n",
    "\n",
    "sessions_train_df = df.merge(\n",
    "    sessions_train_df[['user_id', 'track_id', 'score']],\n",
    "    on=['user_id', 'track_id'],\n",
    "    how='left'\n",
    ").fillna(0)\n",
    "\n",
    "sessions_val_df = df.merge(\n",
    "    sessions_val_df[['user_id', 'track_id', 'score']],\n",
    "    on=['user_id', 'track_id'], \n",
    "    how='left'\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d0f47-b73f-40f2-b298-46fb33a6fcc4",
   "metadata": {},
   "source": [
    "### Calculating Preferences and Confidence Weights\n",
    "\n",
    "Following the collaborative filtering algorithm, we transform each user-track interaction into two values:\n",
    "\n",
    "1. Preference:\n",
    "  - Set to 1 if score ≥ 1 (user interacted positively with track)\n",
    "  - Set to 0 if score = 0 (no interaction)\n",
    "\n",
    "2. Confidence Weight:\n",
    "  - Calculated using the formula: 1 + α * log(1 + score/ε)\n",
    "  - Where:\n",
    "    - α (alpha) = 40: hyperparameter controlling confidence scaling\n",
    "    - ε (epsilon) = 1e-8: small constant to prevent log(0)\n",
    "  - Higher interaction scores result in higher confidence values\n",
    "\n",
    "These values are then used in the model's objective function:\n",
    "\n",
    "$$\n",
    "\\min_{x,y} \\sum_{u,i} c_{ui}(p_{ui} - x_u^Ty_i)^2 + \\lambda(\\sum_u ||x_u||^2 + \\sum_i ||y_i||^2)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $c_{ui}$: confidence weight for interaction between user $u$ and track $i$\n",
    "- $p_{ui}$: preference value (0 or 1) for user $u$ and track $i$\n",
    "- $x_u$: embedding vector for user $u$\n",
    "- $y_i$: embedding vector for track $i$\n",
    "- $x_u^Ty_i$: dot product of user and track embeddings (predicted preference)\n",
    "- $\\lambda$: regularization parameter controlling the strength of regularization\n",
    "- $||x_u||^2$, $||y_i||^2$: L2 norms of embedding vectors (regularization terms)\n",
    "\n",
    "In our implementation, we use L2 regularization through the AdamW optimizer's weight decay parameter, which achieves the same effect of preventing overfitting by penalizing large embedding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c915386-c185-4927-8d2f-9ed1ec97f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_train_df['preference'] = sessions_train_df['score'] >= 1\n",
    "sessions_val_df['preference'] = sessions_val_df['score'] >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72688251-409f-46de-95e3-f81ac5a339f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 40\n",
    "epsilon = 1e-8\n",
    "sessions_train_df['weight'] = 1 + alpha * np.log(1 + sessions_train_df['score'] / epsilon)\n",
    "sessions_val_df['weight'] = 1 + alpha * np.log(1 + sessions_val_df['score'] / epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a297b08-a1e0-47a9-9181-3c1b94aa2287",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd2f5f-b277-482d-8954-37c2af386135",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.LongTensor(sessions_train_df['user_id'].values),\n",
    "    torch.LongTensor(sessions_train_df['track_id'].values),\n",
    "    torch.FloatTensor(sessions_train_df['preference'].values),\n",
    "    torch.FloatTensor(sessions_train_df['weight'].values)\n",
    ")\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.LongTensor(sessions_val_df['user_id'].values),\n",
    "    torch.LongTensor(sessions_val_df['track_id'].values),\n",
    "    torch.FloatTensor(sessions_val_df['preference'].values),\n",
    "    torch.FloatTensor(sessions_val_df['weight'].values)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc7882-cf69-49e9-a234-82ca20db6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_loader, val_loader, device)\n",
    "train_loss, val_loss = trainer.train_with_weight_decay(\n",
    "    epochs=15,\n",
    "    learning_rate=0.005,\n",
    "    weight_decay=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e06199-d61b-4ad4-aff7-e36a0abdfd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, 'b-', label='Training Loss')\n",
    "plt.plot(val_loss, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095838fd-2244-497a-90aa-c81f0f52a7ef",
   "metadata": {},
   "source": [
    "### Training Results Analysis\n",
    "\n",
    "The training process shows an interesting and unusual pattern where the validation loss is consistently lower than the training loss. After multiple code reviews and verifications of the implementation, we couldn't identify any bugs that would cause this behavior.\n",
    "\n",
    "This counterintuitive result might be explained by the data characteristics of our problem:\n",
    "1. We're training on all tracks from tracks.json, but users interacted with only a small subset of them (this will be explored later)\n",
    "2. This creates a highly sparse interaction matrix with many zero values\n",
    "3. Combined with:\n",
    "  - The large number of model parameters (embeddings for all users and tracks)\n",
    "  - Relatively small amount of actual interaction data\n",
    "  - The complexity of gradient descent optimization\n",
    "  \n",
    "This sparsity and parameter-to-data ratio could potentially lead to this unusual loss pattern. However, without further investigation, we cannot make a definitive conclusion about the cause of this behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ab17d-b032-4a7d-bf3b-0d009348cc85",
   "metadata": {},
   "source": [
    "### Regularization Analysis\n",
    "\n",
    "To verify if our model's regularization is working correctly, we examined the norms of the learned embeddings:\n",
    "- Average user embedding norm: 0.34\n",
    "- Average item embedding norm: 0.98\n",
    "\n",
    "These values indicate that the regularization (implemented through AdamW's weight decay) is effectively preventing the embeddings from growing too large. This suggests that the unusual training/validation loss pattern is not caused by insufficient regularization, and we should look for other explanations in the data characteristics and model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7a080-a327-474f-b3f7-bf34cfdcfaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_norms = torch.norm(model.user_embedding.weight, dim=1)\n",
    "avg_user_norm = torch.mean(user_norms) \n",
    "item_norms = torch.norm(model.item_embedding.weight, dim=1)\n",
    "avg_item_norm = torch.mean(item_norms)\n",
    "\n",
    "print(\"Average user embedding norm:\", avg_user_norm.item())\n",
    "print(\"Average item embedding norm:\", avg_item_norm.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f153c-627b-47f3-b5cb-69372ca5ae86",
   "metadata": {},
   "source": [
    "### Saving the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753b4aa-9d40-431f-bf38-9848fe93ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'embedding_dim': model.user_embedding.embedding_dim,\n",
    "        'dropout_rate': model.dropout.p,\n",
    "        'bias': model.bias,\n",
    "        'sigmoid': model.sigmoid,\n",
    "    },\n",
    "    'user_encoder': user_encoder,\n",
    "    'track_encoder': track_encoder\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, './model_files/wmf_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae7302-567a-4231-8c22-ea47efe3e4c6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c8dd3-d14e-44b2-8cab-6e51eb306139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.wmf import WMF\n",
    "from data.data_transforming import IDEncoder\n",
    "from evaluation.evaluator import Evaluator\n",
    "from evaluation.metrics import print_metrics\n",
    "from prediction.predict import predict\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8dfb26-ab75-465d-a5e1-5bd1a7670eca",
   "metadata": {},
   "source": [
    "### Use when you would like to load model from file ↓ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3417963f-98c4-47f1-8d50-9c146da33fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = torch.load(\n",
    "    './model_files/wmf_model.pth',\n",
    "    map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "user_encoder = model_data[\"user_encoder\"]\n",
    "track_encoder = model_data[\"track_encoder\"]\n",
    "config = model_data[\"model_config\"]\n",
    "model = WMF(\n",
    "    n_users=len(user_encoder),\n",
    "    n_items=len(track_encoder),\n",
    "    embedding_dim=config[\"embedding_dim\"],\n",
    "    dropout_rate=config[\"dropout_rate\"],\n",
    "    bias=config[\"bias\"],\n",
    "    sigmoid=config[\"sigmoid\"],\n",
    "    init=False\n",
    ")\n",
    "model.load_state_dict(model_data[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dbe14b-469a-486b-a294-6f94b25b83ae",
   "metadata": {},
   "source": [
    "### Loading the validation data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6dfd3c-1300-4a89-965b-a2fa89bbf8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_val_df = pd.read_json('data_files/val_sessions.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0082d-c709-4852-a02b-eee9925ab596",
   "metadata": {},
   "source": [
    "### Encode the validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0db959-f606-40f9-b334-6739dd9890e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_val_df[\"user_id\"] = user_encoder.encode(sessions_val_df[\"user_id\"].tolist())\n",
    "sessions_val_df[\"track_id\"] = track_encoder.encode(sessions_val_df[\"track_id\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155bf44a-ff03-4966-8727-86672c33d5ce",
   "metadata": {},
   "source": [
    "### Preparing Evaluation Data\n",
    "\n",
    "For evaluation purposes, we select a random subset of 50 users. For these users, we generate all possible user-track pairs to evaluate how well our model ranks all available tracks for each user.\n",
    "\n",
    "The process involves several steps:\n",
    "1. Select 50 random users from our user base using numpy's random choice function\n",
    "2. Create a cartesian product of these users with all available tracks in our dataset\n",
    "3. Merge this complete set of user-track pairs with our validation data (which contains actual user interactions)\n",
    "4. Fill missing values with 0 (representing no interaction)\n",
    "\n",
    "This approach assumes that the validation data (`sessions_val_df`) has already been aggregated using the `aggregate_interactions` function, which was done in the data preparation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533dc57d-1b4e-4898-b644-b315757728ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ids = np.random.choice(len(user_encoder), size=50, replace=False)\n",
    "users = np.array(random_ids)\n",
    "tracks = np.arange(len(track_encoder))\n",
    "df = pd.DataFrame(\n",
    "    product(users, tracks),\n",
    "    columns=['user_id', 'track_id']\n",
    ")\n",
    "\n",
    "sessions_val_df = df.merge(\n",
    "    sessions_val_df[['user_id', 'track_id', 'score']],\n",
    "    on=['user_id', 'track_id'], \n",
    "    how='left'\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a446f23-bee3-4608-8f91-df9d772a5b83",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140dce07-0aa9-473f-ae15-f39ffc0e1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.LongTensor(sessions_val_df['user_id'].values),\n",
    "    torch.LongTensor(sessions_val_df['track_id'].values),\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cb7a2-8dcc-4796-a9a1-406d7aed3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_val_df['score_pred'] = predict(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836ee7c-0b5a-4c70-948b-597de1c21c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [5, 10, 30]\n",
    "evaluator = Evaluator(k_list, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24749618-bee7-43cf-b1dd-927d6c6c64c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluator.evaluate(sessions_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad63a1-43de-4442-af53-3fb7a075599f",
   "metadata": {},
   "source": [
    "## Overall Model Performance\n",
    "Looking at the mean metric values, we can see that the model performs only slightly better than random recommendations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ee510-487e-45eb-8f53-208e851d3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(metrics, k_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8cae4-a935-4abe-b7bf-8987c5a6c09b",
   "metadata": {},
   "source": [
    "## Per user metrics display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d575a3b3-4990-427e-ad17-d30be64060c9",
   "metadata": {},
   "source": [
    "To investigate model performance, we displayed metrics for each individual user. The results showed that most users had metrics equal to zero, while a few users (e.g., user 979 with NDCG@5 = 0.619 and Precision@5 = 1.0) showed exceptionally high performance. This interesting pattern led us to investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942f1b1-54f9-4058-b1cf-30243267fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame()\n",
    "for k in k_list:\n",
    "    metrics_df[f'NDCG@{k}'] = metrics[f'NDCG@{k}']\n",
    "    metrics_df[f'Precision@{k}'] = metrics[f'Precision@{k}']\n",
    "    metrics_df[f'Recall@{k}'] = metrics[f'Recall@{k}']\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36f456-557b-4676-8239-fbd558461278",
   "metadata": {},
   "source": [
    "### Analysis of Model Performance Variations\n",
    "We compared two users - one with high metrics (user_id 979, NDCG@5 = 0.619) and one with near-zero metrics (user_id 87, NDCG@5 = 0) - to understand why only a few users had high metric scores. Our analysis revealed that for users with high metrics, the tracks they listened to in the validation set were also present in their training set interactions. For users with near-zero metrics, there was little to no overlap between their training and validation tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bff8ed-ce3e-451d-9ca1-af2b740ca0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_val_df['track_id'] = sessions_val_df['track_id'].astype('int64')\n",
    "sessions_train_df['track_id'] = sessions_train_df['track_id'].astype('int64')\n",
    "sessions_val_df['user_id'] = sessions_val_df['user_id'].astype('int64')\n",
    "sessions_train_df['user_id'] = sessions_train_df['user_id'].astype('int64')\n",
    "\n",
    "merged_df = sessions_val_df.merge(\n",
    "    sessions_train_df, \n",
    "    on=['user_id', 'track_id'], \n",
    "    how='right',\n",
    "    suffixes=('_val', '_train')\n",
    ")\n",
    "\n",
    "print(merged_df[merged_df['user_id'] == 87])\n",
    "print(merged_df[merged_df['user_id'] == 979])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63497005-b360-4e0e-bf95-758af1fc1942",
   "metadata": {},
   "source": [
    "## Training Data Sparsity Analysis\n",
    "The heatmap visualization of user-track interactions reveals a critical limitation in our training data: users only interacted with a very narrow range of tracks (visible as sparse vertical green bands), while the vast majority of tracks had very low or zero interaction counts (dark purple regions).\n",
    "This extreme sparsity is particularly problematic because our model attempts to learn embeddings for all tracks present in tracks.json, regardless of their interaction count. During gradient descent optimization, we're trying to learn meaningful representations for thousands of tracks based on extremely limited interaction data. With such sparse information spread across such a large parameter space (embeddings for all tracks), the model struggles to learn meaningful patterns that would generalize well to recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd51783-7935-4363-ba98-856d7c60c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "heatmap_data = sessions_train_df.pivot_table(\n",
    "    values='score',\n",
    "    index='user_id',\n",
    "    columns='track_id',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.pcolormesh(heatmap_data, cmap='viridis')\n",
    "plt.colorbar(label='Score')\n",
    "plt.title('Heatmap of User-Track Scores')\n",
    "plt.xlabel('Track ID')\n",
    "plt.ylabel('User ID')\n",
    "\n",
    "plt.xticks(np.arange(0, len(heatmap_data.columns), 100), \n",
    "          heatmap_data.columns[::100], \n",
    "          rotation=45)\n",
    "plt.yticks(np.arange(0, len(heatmap_data.index), 100), \n",
    "          heatmap_data.index[::100])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200f735-196e-454a-b672-5f3d412df529",
   "metadata": {},
   "source": [
    "## Model Performance Analysis Conclusion\n",
    "\n",
    "During consulations, we identified two likely reasons for the model's poor performance:\n",
    "\n",
    "Our training dataset (~500MB) was several orders of magnitude smaller than what's typically needed for effective collaborative filtering (tens of gigabytes)\n",
    "The extreme sparsity of user-track interactions, where we attempted to learn embeddings for all tracks in tracks.json despite most tracks having very few interactions\n",
    "\n",
    "These limitations likely prevented the model from learning meaningful user and track embeddings, explaining why most users had near-zero performance metrics except for a few cases where there was substantial overlap between training and validation interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829bce03-babf-42fe-8da8-80b02285cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.core.advanced_model import AdvancedModel\n",
    "advanced_model = AdvancedModel('model_files/wmf_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
